## 5.1 집계

### 5.1.1 엘라스틱서치와 데이터 분석
일반적인 통게 프로그램은 배치 방식으로 데이터를 처리한다. 대용량 데이터를 하둡이나 관계형 데이터베이스에 적재하고 배치로 처리하는 식이다.  
반면 엘라스틱 서치는 많은 양의 데이터를 조각내어 관리한다. 덕분에 문서의 수가 늘어나도 배치 처리보다 좀 더 실시간에 가깝게 문서를 처리할 수 있다.  

SQL로는 다음과같이 GROUP BY 구문을 이용해 집계할 수 있다.
~~~
SELECT SUM(ratings) FROM movie_review GROUP BY movie_no;
~~~
엘라스틱 서치의 Query DSL로 집계하는 쿼리는 다음과 같다.
~~~
POST /movie_search/_search
{
  "aggs": {
    "movie_no_agg": {
      "terms": {
        "field": "movie_no"
      },
      "aggs": {
        "ratings_agg": {
          "sum": {
            "field": "ratings"
          }
        }
      }
    }
  }
}
~~~
엘라스틱서치는 SQL보다 더욱 강력한 집계 기능을 제공한다.  
집계를 여러 개 중첩해 사용할 수 있을뿐더러 범위, 날짜, 위치 정보도 집꼐할 수 있다.  
또한 엘라스틱서치는 인덱스를 활용해 분산 처리가 가능하기 때문에 SQL보다 더 많은 데이터를 빠르게 집계할 수 있다.

### 5.1.2 엘라스틱서치가 집계에 사용하는 기술
#### 캐시  
집계 쿼리로 값을 조회하면, 여러 노드에 있는 데이터를 집계해 질의에 답하기 때문에 데이터의 양이 클수록 CPU와 메모리 자원이 많이 소모된다.  
질의에 응답하는 시간 또한 길어진다.  
이런 문제는 노드의 하드웨어 성능을 높이면 해결할 수 있지만 하드웨어 업그레이드는 막대한 비용이 들기 때문에 올바른 해결책이 아니다.  
가장 현실적이고 비용 효율적인 결책은 '캐시'를 이용하는 것이다.  
캐시는 질의결과를 임시 버퍼(캐시)에 둔다. 이후 처리해야 하는 같은 질의에 대해 매번 결과를 계산하는 게 아니라 버퍼에 보관된 결과를 반환한다.  
캐시를 적용하는 것만으로도 인덱스의 성능을 대폭 향상시킬 수 있다.  
캐시의 크기는 일반적으로 힙 메모리의 1%를 할당하며, 캐시에 없는 질의의 경우 성능 향상에 별다른 도움이 되지 않는다.  

캐시는 엘라스틱서치의 conf 폴더 안의 elasticsearch.yml 파일을 수정해 활성화할 수 있다.  
다음은 힙메모리의 2%를 캐시에 할당하는 예다.
~~~
indices.requests.cache.size: 2%
~~~

- Node query Cache  
노드의 모든 샤드가 공유하는 LRU(Least-Recently-Used) 캐시다. 캐시 용량이 가득차면 사용량이 가장 적은 데이터를 삭제하고 새로운 결괏값을 캐싱한다.  
기본적으로 10%의 필터 캐시가 매모리를 제어하고 쿼리 캐싱을 사용할지 여부는 elasticsearch.yml 파일에 옵션으로 추가하면 된다. 기본값은 true
~~~
index.queries.cache.enabled:true
~~~

- Shard request Cache  
엘라스틱서치는 인덱스의 수평적 확산과 병렬 처리를 통한 성능 향상을 위해 고안된 개념이다.  
샤드는 데이터를 분산 저장하기 위한 단위로서, 그 자체가 온전한 기능을 가진 독립 인덱스라고 할 수 있다.  
- Shard request Cache는 바로 이 샤드에서 수행된 쿼리의 결과를 캐싱한다.   
샤드의 내용이 변경되면 캐시가 삭제되기 때문에 업데이트가 빈번한 인덱스에서는 오히려 성능 저하를 일으킬 수 있다.

- Field data Cache  
엘라스틱서치가 필드에서 집계 연산을 수행할 때는 모든 필드 값을 메모리에 로드한다.  
이러한 이유로 엘라스틱서치에서 계산되는 집계 쿼리는 성능적인 측면에서 비용이 많이든다.  
Field data Cache는 집계가 계산되는 동안 필드의 값을 메모리에 보관한다.

### 5.1.3 실습 데이터 살펴보기
스냅숏의 목록을 확인해본다.
~~~
curl -XGET 'http://localhost:9200/_snapshot/apache-web-log/_all?pretty'
~~~
apache-web-log 스냅숏 그룹 내부에는 다음과 같이 2개의 스냅숏이 존재한다.
~~~
{
  "snapshots": [
    {
      "snapshot": "default",
      "uuid": "yzmzEx6uSMS55j60z4buBA",
      "repository": "apache-web-log",
      "version_id": 6040399,
      "version": "6.4.3",
      "indices": [
        "apache-web-log"
      ],
      "data_streams": [],
      "include_global_state": false,
      "state": "SUCCESS",
      "start_time": "2019-03-23T16:03:50.351Z",
      "start_time_in_millis": 1553357030351,
      "end_time": "2019-03-23T16:03:50.604Z",
      "end_time_in_millis": 1553357030604,
      "duration_in_millis": 253,
      "failures": [],
      "shards": {
        "total": 5,
        "failed": 0,
        "successful": 5
      },
      "feature_states": []
    },
    {
      "snapshot": "applied-mapping",
      "uuid": "SgXhqApiSHiauC6fbjSHMw",
      "repository": "apache-web-log",
      "version_id": 6040399,
      "version": "6.4.3",
      "indices": [
        "apache-web-log-applied-mapping"
      ],
      "data_streams": [],
      "include_global_state": false,
      "state": "SUCCESS",
      "start_time": "2019-03-23T16:05:46.038Z",
      "start_time_in_millis": 1553357146038,
      "end_time": "2019-03-23T16:05:46.364Z",
      "end_time_in_millis": 1553357146364,
      "duration_in_millis": 326,
      "failures": [],
      "shards": {
        "total": 5,
        "failed": 0,
        "successful": 5
      },
      "feature_states": []
    }
  ],
  "total": 2,
  "remaining": 0
}
~~~
이 중에서 default 스냅숏을 복구한다. 터미널에 다음명령을 실행한다.
~~~
curl -XPOST 'http://localhost:9200/_snapshot/apache-web-log/default/_restore'
~~~
인덱스가 잘 생성됐는지 확인해보자.
정상적으로 생성됐다면 다음과 같이 apache-web-log 인덱스가 생성되며 총 10,001건의 로그가 존재한다.
~~~
GET /_cat/indices/apache*?v&pretty

health status index          uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   apache-web-log zkKZfOw6RaiFKPM1iMCbkQ   5   1      10001            0      8.8mb          8.8mb
~~~
맨 먼저 집계해볼 데이터는 지역별 사용자의 접속 수다. 
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "match_all": {
    }
  },
  "aggs": {
    "region_count": {
      "terms": {
        "field": "geoip.region_name.keyword",
        "size": 20
      }
    }
  }
}
~~~
집계에서 사용하는 필드 중 문자열 형태의 필드를 사용한다면 Keyword 타입으로 지정해야 한다.  
Keyword 타입은 Text 타입과 달리 분석 과정을 수행하지 않기 때문에 집계 성능이 향상된다.  
예제에서는 geoip.region_name.keyword와 같이 지역명에 대해 Keyword 타입을 지정했다.

집계쿼리를 실행한 결과는 다음과 같다.
~~~
{
  "took": 131,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 10000,
      "relation": "gte"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "region_count": {
      "doc_count_error_upper_bound": 39,
      "sum_other_doc_count": 3548,
      "buckets": [
        {
          "key": "California",
          "doc_count": 756
        },
        {
          "key": "Texas",
          "doc_count": 588
        },
        {
          "key": "Virginia",
          "doc_count": 424
        },
        {
          "key": "Pennsylvania",
          "doc_count": 355
        },
        {
          "key": "Washington",
          "doc_count": 273
        }
        ... (생략) ...,       
      ]
    }
  }
}
~~~
집계를 사용하면 쉽고 빠르게 필드를 그룹으로 묶고 통계 결과를 도출할 수 있다.

### 5.1.4 Aggregation API 이해하기
서비스를 운영하다 보면 데이터 필드의 값을 더하거나 평균을 내는 등 검색 쿼리로 반환된 데이터를 집계하는 경우가 많다.  
검색 쿼리의 결과 집계는 다음과 같이 기존 검색 쿼리에 집계 구문을 추가하는 방식으로 수행할 수 있다.
~~~
{
    "query": {...생략...},
    "aggs": {...생략...}
}
~~~
엘라스틱 서치는 집계 시 문서를 평가한 후 기준에 만족하는 문서들을 하나로 그룹화한다.  
그룹화한 집합을 토대로 집계를 수행하고, 집계가 끝나면 다음과 같이 버킷 목록에 속한 문서의 집합이 출력된다.
~~~
...생략...
"aggregations": {
    "region_count": {
      "doc_count_error_upper_bound": 39,
      "sum_other_doc_count": 3548,
      "buckets": [
        {
          "key": "California",
          "doc_count": 756
        },
        {
          "key": "Texas",
          "doc_count": 588
        },
        {
          "key": "Virginia",
          "doc_count": 424
        },
        {
          "key": "Pennsylvania",
          "doc_count": 355
        },
        {
          "key": "Washington",
          "doc_count": 273
        }
        ... (생략) ...,       
      ]
    }
  }
}
~~~
~~~
- 버킷 집계: 쿼리 결과로 도출된 도큐먼트 집합에 대해 특정 기준으로 나눈 다음 나눠진 도큐먼트들에 대한 산술 연산을 수행한다.
이때 나눠진 도큐먼트들의 모음들이 각 버킷에 해당된다.

- 메트릭 집계: 쿼리 결과로 도출된 도큐먼트 집합에서 필드의 값을 더하거나 평균을 내는 등의 산술 연산을 수행한다.

- 파이프라인 집계: 다른 집계 또는 관련 메트릭 연산의 결과를 집계한다.

- 행렬 집계: 버킷 대상이 되는 도큐먼트의 여러 필드에서 추출한 값으로 행렬 연산을 수행한다. 이를 토대로 다양한 통계정보를 제공하고 있으나
아직은 공식적인 집계 연산으로 제공되지 않고 실험적인 기능으로 제공되기 때문에 사용할 때 주의해야 한다.
~~~
엘라스틱 서치가 강력한 이유는 집계를 중첩해서 사용할 수 있다는 데 있다.  
하위 집계가 상위 집계의 버킷을 다시 집계하는 식이다.  
예컨데 상위 집계에서 date_histogram 집계로 일자별로 집계한 후 그 결과를 메트릭 집계로 다시 합산해 결과를 도출할 수 있다.  
중첩 횟수에 제한은 없으나 중첩할수록 성능 하락이 뒤따른다는 점에 주의해야 한다.

- 집계구문의 구조  
엘라스틱서치에서 제공하는 집계 연산을 사용하기 위해서는 문법적인 구조를 이해해야 한다.  
기본적인 집계 구문의 구조를 알아보자. 기본적인 구조는 다음과 같다.
~~~
"aggreagtions" : {
    "<aggregation_name>" : {
        "<aggregation_type>" : {
            <aggregation_body>
        }
        [,"meta" : { [ <meta_data_body> ] } ]?
        [, "aggregations" : { [ <sub_aggregation> ]+ }]?
    }
    [,"<aggregation_name_2>" : { ... } ]*
}
~~~
데이터를 집계하기 위해서는 맨 먼저 "aggregations"라는 단어를 명시해야 한다.  
"aggregations" 대신 "aggs"로 줄여서 쓰는 것도 가능하다.
"aggregation_name"에는 하위 집계의 이름을 기입한다. 이 이름은 집계의 결과 출력에 사용된다. 따라서 사용자가 직접 적당한 임의의 이름을 지정하다.  

"aggregation_type"에는 집계의 유형을 적는다. 어떤 집계를 수행할 것인가를 나타내는데, terms,date_histogram, sum 과 같은 다양한 집계 유형을 지정할 수 있다.  
"aggregation_body"에는 앞서 지정한 aggregation_type에 맞춰 내용을 작성하면 된다.

또한 meta 필드를 사용하거나 aggregations를 중첩할 수 있는데, 중첩의 경우 같은 레벨(aggregation_name_2)에 또 다른 집계를 정의하는 것도 가능하다.  
단, 같은 레벨에 집계를 정의할 때는 부수적인 성격의 집계만 정의할 수 있다.

- 집계 영역(Aggregation Scope)  
집계를 질의와 함계 수행하면 질의의 결과 영역 안에서 집계가 수행된다. 즉, 질의를 통해 반환된 문서들의 집합 내에서 집계를 수행하게 된다.
~~~
{
1."query": {
    "constant_score": {
        "filter" : {
            "match" : <필드 조건>
        }
    }
  },
2."aggs": {
    "<집계 이름>": {
      "<집계 타입>": {
        "field": "<필드명>"
      }
    }
  }
}
~~~
1. query: 질의를 수행한다. 하위에 필터 조건에 의해 명시한 필드와 값이 일치하는 문서만 반환한다.
2. aggs: 질의를 통해 반환받은 문서들의 집합 내에서 집계를 수행한다.

만약 질의가 생략된다면 내부적으로 match_all 쿼리로 수행되어 전체 문서에 대해 집계가 수행된다.
~~~
{
1."size" : 0,
2.  "aggs" : {
      "<집계 이름>": {
          "<집계 타입>": {
            "field": "<필드명>"
          }
      }
  }
}
~~~
1. size: 질의가 명시돼 있지 않기 때문에 내부적으로는 match_all 이 수행되고 size가 0이기 때문에결과 집합에 문서들 또한 존재하지 않는다. 즉, 문서의 결과는 출력되지 않는다.
2. aggs: 결과 문서가 출력되지 않더라도 실제 검색된 문서의 대상 범위가 전체 문서이기 때문에 집계는 전체 문서에 대해 수행된다.

한 번의 집계를 통해 질의에 해당하는 문서들 내에서도 집계를 수행하고 전체 문서에 대해서도 집계를 수행해야 하는 경우  
글로벌 버킷을 사용하면 질의 내에서도 전체 문서를 대상으로 집계를 수행할 수 있다.
~~~
{
  "query" : {
    "constant_score" : {
      "filter" : {
        "match" : <필드 조건>
      }
    }     
  },
1."aggs" : {
    "<집계 이름>" : {
      "<집계 타입>" : {
        "field" : "<필드명>"
      }
    },
    "<집계 이름>" : {
2.    "global" : {},
      "aggs" : {
        "<집계 이름>" : {
          "<집계 타입>" : {
            "field" : "<필드명>"
          }
        }
      }
    }
  }
}
~~~

1. 일반 버킷: 질의 영역 내에서만 집계를 수행
2. 글로벌 버킷: 전체문서를 대상으로 집계를 수행

## 5.2 메트릭 집계
메트릭 집계(Metrics Aggregations)를 사용하면 특정 필드에 대해 합이나 평균을 계산하거나  
다른 집계와 중첩해서 결과에 대해 특정 필드의 _score 값에 따라 정렬을 수행하거나  
지리 정보를 통해 범위 계산을 하는 등의 다양한 집계를 수행할 수 있다. 정수 또는 실수와 같이 숫자 연산을 할 수 있는 값들에 대한 집계를 수행한다.  
일반적으로 필드 데이터를 사용해 집계가 이뤄지지만 스크립트를 사용해 조금 더 유연하게 집계를 수행할 수도 있다.

메트릭 집계 내에서도 단일 숫자 메트릭 집계 와 다중 숫자 메트릭 집계로 나뉘는데,  
단일 숫자 메트릭 집계는 집계를 수행한 결괏값이 하나라는 의미로서 sum과 avg 등이 이에 속한다.  
다중 숫자 메트릭 집계는 집계를 수행한 결괏값이 여러 개가 될 수 있고, stats 나 geo_bounds가 이에 속한다.

예제에 사용된 쿼리는 전부 전체 데이터에 대해 집계를 수행하는데, 전체 데이터에 대한 쿼리라면 "query" 부분을 생략할 수 있다.  
또한 집계 결과가 아닌 검색 결과의 내용을 볼 필요가 없는 경우에는 size 값을 0으로 지정해서 검색 결과가 반환되지 않게 할 수도 있다.  
예제에서는 이런 방식을 사용해서 불필요한 결과는 노출하지 않게 할 것이다.

집계 요청에 사용될 공통적인 구조를 살펴본다.
~~~
1.  GET /apache-web-log/_search?size=0
    {
2.    "aggs": {
3.      "<집계 이름>": {
4.        "<집계 타입>": {
5.          "field": "<필드명>"
          }
        }
      }
}
~~~
1. size: 집계된 문서들의 데이터는 불필요하므로 size 값을 0으로 지정해 반환받지 않는다.
2. aggs: 집계를 수행한다. aggregation 또는 aggs를 입력할 수 있다.
3. 집계 이름: 집계에 대한 이름이다. 하위 쿼리 또는 여러 쿼리를 함께 사용할 때 구별하는 용도로 사용한다.
4. 집계 타입: 합계, 평균, 시계열 드으이 집계 타입을 명시한다.
5. field: 집계의 대상이 되는 필드를 명시한다.

집계 결과의 공통적인 구조를 살펴본다.
~~~
    {
1.    "took": 1,
2.    "time_out": false,
3.    "_shard": {
4.      "total": 5,
5.      "successful": 5,
6.      "skipped": 0,
7.      "failed": 0,
      },
8.    "hits": {
9.      "total": 200,
10.       "max_score": 0,
11.       "hits": [ ]
      },
12.     "aggreations": {
13.       "<집계 이름>": {
14.         <집계 결과>
        }
      }
    }
~~~
1. took: 엘라스틱서치가 검색을 실행하는 데 소요된 시간(ms)
2. time_out: 검색시간이 초과됐는지 여부
3. _shard: 검색에 영향받은 샤으데 대한 정보
4. _shard.total: 검색에 영향받은 샤드의 총 개수
5. _shard.successful: 검색 요청에 대한 처리를 정상 수행한 샤드 수
6. _shard.skipped: 검색 요청에 대한 처리를 건너 뛴 샤드 수
7. _shard.failed: 검색 요청에 대한 처리를 실패한 샤드 수
8. hits: 검색 결과
9. hits.total: 검색 기준과 일치하는 총 문서 수
10. hits.max_score: 검색 결과에 포함된 문서의 최대 스코어 값
11. hits.hits: 검색 결과 문서들의 배열(기본적으로 10개의 문서를 반환)
12. aggregations:집계 결과
13. 집계 이름:검색을 요청할 때 지정한 집계의 이름
14. 집계 결과:검색을 요청할 때 지정한 집계 타입에 따른 결과

### 5.2.1 합산 집계
합산집계(Sum Aggregation)는 단일 숫자 메트릭 집계에 해당한다.  
이를 통해 해당 서버로 총 얼마만큼의 데이터가 유입됐는지 집계해 보자.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "total_bytes": {
      "sum": {
        "field": "bytes"
      }
    }
  }
}
~~~
다음과 같이 집계 결과가 반환된다. 이를 통해 현재 서버로 총 얼마만큼의 데이터(Bytes)가 유입됐는지 쉽게 확인할 수 있다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "total_bytes": {
      "value": 2.747282505E9
    }
  }
}
~~~

이번에는 filter 기능을 사용해 특정 지역에서 유입된 데이터의 합을 계산해 본다.
~~~
GET /apache-web-log/_search?size=0
{
1."query": {
2.  "constant_score": {
3.    "filter": {
4.      "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "total_bytes": {
      "sum": {
        "field": "bytes"
      }
    }
  }
}
~~~
1. query: 쿼리 컨텍스트를 의미한다.
2. constant_score: 필터에 해당하는 문서들에 대해 동일한 스코어를 부여한다.
3. filter: 필터 컨텍스트를 의미한다.
4. match: geoip.city_name 필드 값이 'Paris'인 문서를 검색한다.

결과를 확인해본다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "total_bytes": {
      "value": 428964.0
    }
  }
}
~~~
필터 쿼리를 사용하면 위와 같이 파리에서 유입된 데이터 총량을 간단하게 확인할 수 있다.  
유입되는 데이터가 많아질수록 데이터의 크기가 커질 것이기 때문에 나중에는 데이터 총량이 어마어마하게 커질수도 있다.  
이럴 때 KB나 MB, GB 단위로 보고싶다면 script를 활용하여 집계되는 데이터를 원하는 단위로 변환할 수 있다.

다음과 같이 script를 사용해 합 연산을 수행해보자.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "total_bytes": {
      "sum": {
1.      "script": {
2.        "lang": "painless",
          "source": "doc.bytes.value"
        }
      }
    }
  }
}
~~~
1. script: 스크립트 컨텍스트를 의미하며, 6.x 부터는 그루비(Groovy)가 페인리스(Painless)를 기본 언어로 사용한다.
2. lang: 페인리스언어를 사용하는 경우 기본값이기 때문에 따로 명시하지 않아도 되지만 위 예제에서는 언어 확인을 위해 명시했다.
3. source: bytes 필드를 사용하려면 doc 객체의 bytes 속성 변수를 사용해야 하고, 값을 얻기 위해 bytes 객체의 value 속정변수를 사용해야 한다. 이에 대해서는 페인리스부분에서 자세히 다룬다.

결과는 앞에서 살펴본 합산 집계외 동일하다.

script를 사용하면 기존에 합산만 수행했던 것에서 더 나아가 다양한 연산을 수행할 수 있다.  
여기서는 KB로 나타내기 위해 1000으로 나눌 것이다(정확히는 1024로 나눠야 하지만 계산을 간단히 하기 위해 1000으로 나눈다).
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "total_bytes": {
      "sum": {
        "script": {
          "lang": "painless",
1.        "source": "doc.bytes.value / params.divide_value",
2.        "params": {
3.          "divide_value": 1000
          }
        }
      }
    }
  }
}
~~~
1. byte 필드의 값을 script 내의 params(변수와 같은 의미)에 명시한 값으로 나눈다.
2. script 내에서 사용할 파라미터 값들을 정의한다.
3. divide_value 파라미터의 값으로 1000을 대입한다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "total_bytes": {
      "value": 422.0
    }
  }
}
~~~
기존에는 결과가 428964 였는데 422가 됐다.  
1000으로 나눴을 때 예상했던 값은 428이었는데 422가 됐다. 그 이유는 1000으로 나누는 것은 모든 합산 값에 대한 나누기가 아니라  
각 문서의 개별적인 값을 1000으로 나눈 것이기 때문에 1000보다 작은 수들은 전부 0이 됐기 때문이다.  

이 문제를 해결하기 위해서는 정수가 아닌 실수로 계산해서 소수점까지 합산해야 한다. 이를 위해 다음과 같이 캐스팅 연산을 수행한다.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "total_bytes": {
      "sum": {
        "script": {
          "lang": "painless",
          "source": "doc.bytes.value / (double)params.divide_value",
          "params": {
            "divide_value": 1000
          }
        }
      }
    }
  }
}
~~~
params.divide_value 값을 double 로 형변환 했다. 결과는 다음과 같다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "total_bytes": {
      "value": 428.96399999999994
    }
  }
}
~~~
이처럼 script를 이용하면 집계 시 더 다양한 연산을 추가적으로 수행할 수 있기 때문에 유용하다.

### 5.2.2 평균 집계
평균 집계는 단일 숫자 메트릭 집계에 해당한다.  
이를 통해 해당 서버로 유입된 데이터의 평균 값을 쉽게 구할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "avg_bytes": {
      "avg": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
 ... 생략 ...
  "aggregations": {
    "avg_bytes": {
      "value": 294456.8601286174
    }
  }
}
~~~

이번에는 filter 기능을 사용해 특정 지역에서 유입된 데이터의 합을 계산해 보자.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "avg_bytes": {
      "avg": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "avg_bytes": {
      "value": 20426.85714285714
    }
  }
}
~~~

### 5.2.3 최솟값 집계
최솟값 집계(Min Aggregation)는 단일 숫자 메트릭 집계에 해당한다.  
서버로 유입된 데이터중 가장 작은 값을 쉽게 구할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "min_bytes": {
      "min": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "min_bytes": {
      "value": 35.0
    }
  }
}
~~~

이번에는 filter 기능을 사용해 특정 지역에서 유입된 데이터 중 가장 작은 값을 찾아본다.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "min_bytes": {
      "min": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "min_bytes": {
      "value": 1015.0
    }
  }
}
~~~

### 5.2.4 최댓값 집계
최댓값 집계(Max Aggregations)는 단일 숫자 메트릭 집계에 해당한다. 서버로 유입된 데이터중 가장 큰 값을 쉽게 구할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "max_bytes": {
      "max": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "max_bytes": {
      "value": 6.9192717E7
    }
  }
}
~~~

### 5.2.5 개수 집계
개수 집계(Value Count Aggregation)는 단일 숫자 메트릭 집계에 해당한다. 이를 통해 해당 서버로 사용자 요청이 몇 회 유입됐는지 쉽게 구할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "bytes_count": {
      "value_count": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_count": {
      "value": 9330
    }
  }
}
~~~

### 5.2.6 통계 집계
통계 집계(Stats Aggregation)는 결괏값이 여러 개인 다중 숫자 메트릭 집계에 해당한다.  
통계 집계를 사용하면 앞서 살펴본 합, 평균, 최대/최솟값, 개수를 한번의 쿼리로 집계할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "bytes_stats": {
      "stats": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과는 다음과 같다..(열받네 이걸 이제알려주네 ㅡㅡ)
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_stats": {
      "count": 9330,
      "min": 35.0,
      "max": 6.9192717E7,
      "avg": 294456.8601286174,
      "sum": 2.747282505E9
    }
  }
}
~~~
이번에는 filter 기능을 사용해 특정 지역에 대한 통계를 집계해 보자.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "bytes_stats": {
      "stats": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_stats": {
      "count": 21,
      "min": 1015.0,
      "max": 53270.0,
      "avg": 20426.85714285714,
      "sum": 428964.0
    }
  }
}
~~~

### 5.2.7 확장 통계 집계
확장 통계 집계(Extended Stats Aggregation)는 결괏값이 여러 개인 다중 숫자 메트릭 집계에 해당한다.  
앞서 살펴본 통계 집계를 확장해서 표준편차 같은 통곗값이 추가됐다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "bytes_extended_stats": {
      "extended_stats": {
        "field": "bytes"
      }
    }
  }
}
~~~
쿼리를 수행하면 다음과 같이 여러개의 결과를 포함한 집계가 반환된다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_extended_stats": {
      "count": 9330,
      "min": 35.0,
      "max": 6.9192717E7,
      "avg": 294456.8601286174,
      "sum": 2.747282505E9,
1.    "sum_of_squares": 1.18280314234513344E17,
2.    "variance": 1.2590713617814016E13,
      "variance_population": 1.2590713617814016E13,
      "variance_sampling": 1.2592063249459188E13,
3.    "std_deviation": 3548339.5578515334,
      "std_deviation_population": 3548339.5578515334,
      "std_deviation_sampling": 3548529.730671449,
4.    "std_deviation_bounds": {
5.      "upper": 7391135.975831684,
6.      "lower": -6802222.25557445,
        "upper_population": 7391135.975831684,
        "lower_population": -6802222.25557445,
        "upper_sampling": 7391516.321471516,
        "lower_sampling": -6802602.60121428
      }
    }
  }
}
~~~
1. sum_of_squares: 제곱합을 의미한다. 제곱합은 변동의 측정이나 평균의 편차를 나타낸다.
2. variance: 확률변수의 분산을 의미한다. 분산은 확률 변수가 기댓값으로부터 얼마나 떨어진 곳에 분포하는지를 가늠 하는 숫자다(편차 제곱의 평균.)
3. std_deviation: 표준편차를 의미한다. 표준편차는 자료의 산포도를 나타내는 수치로서, 분산의 양의 제곱근으로 정의된다.
4. std_deviation_bounds: 표준편차의 번위를 의미한다.
5. upper: 표준편차의 상한 값을 의미한다.
6. lower: 표준편차의 하한 값을 의미한다.

이번에는 filter 기능을 사용해 특정 지역에 대한 통계를 집계해 보자.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "bytes_extended_stats": {
      "extended_stats": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_extended_stats": {
      "count": 21,
      "min": 1015.0,
      "max": 53270.0,
      "avg": 20426.85714285714,
      "sum": 428964.0,
      "sum_of_squares": 1.8371748404E10,
      "variance": 4.575886693605442E8,
      "variance_population": 4.575886693605442E8,
      "variance_sampling": 4.8046810282857144E8,
      "std_deviation": 21391.32229107271,
      "std_deviation_population": 21391.32229107271,
      "std_deviation_sampling": 21919.582633539612,
      "std_deviation_bounds": {
        "upper": 63209.501725002556,
        "lower": -22355.787439288277,
        "upper_population": 63209.501725002556,
        "lower_population": -22355.787439288277,
        "upper_sampling": 64266.02240993637,
        "lower_sampling": -23412.308124222083
      }
    }
  }
}
~~~

### 5.2.8 카디널리티 집계
카디널리티 집계(Cardinality Aggregation)는 단일 숫자 메트릭 집계에 해당한다.  
개수 집합과 유사하게 횟수를 계산하는데, 중복된 값은 제외한 고유한 값에 대한 집계를 수행한다.  
하지만 모든 문서에 대해 중복된 값을 집계하는 것은 성능에 큰 영향을 줄 수 있기 때문에 근사치를 통해 집계를 수행한다.

아파치 웹 로그 예제에서 미국의 몇 개 도시에서 데이터 유입이 있었는지 횟수를 집계해 보자.  
먼저 아직 살펴보진 않았지만 terms 집계를 통해 미국의 어느 지역에서 데이터 유입이 있었는지 확인해 보겠다.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
1.      "match": {
          "geoip.country_name": {
            "query": "United States"
          }
        }
      }
    }
  },
  "aggs": {
    "us_city_names": {
2.    "terms": {
3.      "field": "geoip.city_name.keyword"
      }
    }
  }
}
~~~
1. match: 국가명이 "United States"인 문서만 필터링한다.
2. terms: 텀즈 쿼리를 사용한다.
3. field: 개수를 집계할 필드를 명시한다. 각 문서에 명시된 필드별로 개수가 집계된다.  
string 타입의 필드의 경우 text 타입과 keyword 타입의 두 타입이 공존하게 되는데, text 타입의 경우 텍스트 검색용으로 사용하기 위해  
분석기를 수행한 후 색인되기 때문에 집계할 때는 keyword 타입을 사용한다.

쿼리를 수행하면 다음과 같이 각 지역별로 개수가 집계되어 반환된다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "us_city_names": {
      "doc_count_error_upper_bound": 30,
      "sum_other_doc_count": 1955,
      "buckets": [
        {
          "key": "Leander",
          "doc_count": 539
        },
        {
          "key": "Lititz",
          "doc_count": 273
        },
        {
          "key": "San Francisco",
          "doc_count": 230
        },
        {
          "key": "Ashburn",
          "doc_count": 153
        },
        ... 생략 ...
      ]
    }
  }
}
~~~
집계된 결과를 살펴보면 미국 내에서 요청 수가 가장 많은 도시 순으로 결과가 반환된다.  
그렇다면 미국 내 몇 개의 도시에서 유입이 있었는지 확인하고 싶다면 어떻게 해야할까?  
지금까지의 집계로는 동일한 필드 값에 대해서도 집계 연산이 수행되기 때문에 알아낼 수가 없었다.  
이러한 경우는 다음과 같이 카디널리티 집계를 사용한다.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.country_name": {
            "query": "United States"
          }
        }
      }
    }
  },
  "aggs": {
    "us_cardinality": {
      "cardinality": {
        "field": "geoip.city_name.keyword"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "us_cardinality": {
      "value": 249
    }
  }
}
~~~
앞에서 언급한 것처럼 카디널리티 집계는 대상 필드를 고유한 값으로 근사치를 계산하는 메트릭 집계다.  
데이터가 적은 경우 거의 정확한 결과를 확인할 수 있지만 기본적으로 근사치 계산이라는 점에 주의하자.  
위 결과를 통해 미국 내 249개의 도시에서 요청이 있었음을 알 수 있다.


---
**참고**  
근사치 계산(Approximate) 계산  
카디널리티 집계는 HyperLogLog++ 알고리즘을 기반으로 동작한다. 이 알고리즘은 해시를 기반으로 계산되며, 다음과 같은 특성이 있다.
- 정확성을 위해 메모리를 교환하는 방법을 결정한다. 이는 정확도를 높일수록 더 많은 메모리가 필요하다는 것을 의미한다.
- 카디널리티가 낮은 집합일수록 더 뛰어난 정확성을 보인다.
- 수십억 개의 고윳값이 존재하더라도 메모리 사용은 정확도(Precision) 설정에 의존해서 고정된 메모리를 사용한다.

여기서 정확도는 precision_threshold 속성에 설정하며 0~40000의 값을 설정할 수 있다. 일반적으로 카디널리티가 precision보다 적은 경우  
거의 100% 정확도를 보인다. 메모리 사용률이 precision_threshold * 8 바이트이기 때문에 워크로드에 따라 적절한 수치를 지정한다.  
앞에서 언급했듯이 근사치 계산은 해시를 기반으로 수행되기 때문에 이 해시값을 미리 계산해 두면 성능 향상에 도움이 된다.  
해시 계산에는 murmur3알고리즘이 사용되는데 이를 사용하기 위해서는 엘라스틱서치에 플러그인이 설치돼 있어야 한다.

murmur3 사용을 위한 플러그인 설치  
$ bin/elasticsearch-plugin install mapper-murmur3  
플러그인이 설치되면 다음과 같이 매핑 설정을 통해 사용할 수 있다.
~~~
{
  "mappings": {
    "hash_settings": {
      "properties": {
        "city_name": {
          "type": "keyword",
          "fields": {
            "hash": {
              "type": "murmur3"
            }
          }
        }
      }
    }
  }
}
~~~
매핑을 설정할 때 근사치 계산이 수행될 필드에 미리 해시 설정을 해두면 색인할 때 해시 값이 미리 계산되기 때문에 집계시 해시 계산이 필요없어지므로 성능이 향상된다
~~~
{
  "aggs": {
    "city_name_cardinality": {
      "cardinality": {
        "field": "city_name.hash"
      }
    }
  }
}
~~~
---

### 5.2.9 백분위 수 집계
백분위 수 집계(Percentiles Aggregation)는 다중 숫자 메트릭 집계에 해당한다.  
백분위 수는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열헸을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어다.  
100명의 학생이 100점 만점의 수학시험을 치른 후 점수를 수집했다고 가정해보자.  
이때 학생들의 점수가 어느 점수 구간에 분포돼 있는지 확인해보면 이번 시험의난이도가 학생들에게 어려웠는지 여부를 명확하게 파악할 수 있다.

이를 아파치 웹 로그에 활용해보면 유입된 데이터가 어느 정도 크기로 분포돼 있는지 확인해볼 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "bytes_percentiles": {
      "percentiles": {
        "field": "bytes"
      }
    }
  }
}
~~~

쿼리를 수행하면 다음과 같이 백분위 수별로 데이터량이 집계되어 반환된다.

~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_percentiles": {
      "values": {
        "1.0": 229.0,
        "5.0": 358.2121212121212,
        "25.0": 3647.165790731008,
        "50.0": 12238.45411431387,
        "75.0": 37580.59330143541,
        "95.0": 171278.54545454544,
        "99.0": 1204031.8000000163
      }
    }
  }
}
~~~
결과를 보면 대부분 만 단위의 크기로 데이터가 유입되지만 120만이 넘는 크기의 데이터가 유입될 때도 있음을 확인할 수 있다.  
앞서 살펴본 최댓값 집계를 사용한다면 빈도수가 아닌 유입된 데이터 중 가장 큰 값이기 때문에 대부분은 더 작은 크기의 데이터로 유입될 수 있어  
데이터 유입 추이를 제대로 살펴볼 수가 없지만 백분위 수를 사용하면 이러한 추이를 명확하게 확인할 수 있다.

이번 예제에 한해 집계 결과를 활용해 본다면 서버 사양보다 너무 큰 데이터가 주로 유입되는 경우 데이터 크기를 조절해서 서비스 품질을 개선할 수 있을 것이다.  
기본적으로 지정되는 백분위 수 외에 다음과 같이 직접 백분위를 입력할 수도 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "bytes_percentiles": {
      "percentiles": {
        "field": "bytes",
        "percents": [
          10,20,30,40,50,60,70,80,90
        ]
      }
    }
  }
}
~~~
결과는 지정한 백분위에 해당하는 결과가 반환된다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_percentiles": {
      "values": {
        "10.0": 1015.0,
        "20.0": 3638.0,
        "30.0": 4877.0,
        "40.0": 8726.448868778281,
        "50.0": 12266.029411764706,
        "60.0": 15887.163349347977,
        "70.0": 29941.0,
        "80.0": 50112.0,
        "90.0": 65748.0
      }
    }
  }
}
~~~
백분위 수 집계 또한 카디널리티 집계와 같이 사치 계산으로 집계된다.  
특정 필드에 대한 백분위 수를 구해야 하기 때문에 카디널리티 집계와는 달리 TDigest 알고리즘을 사용한다.  
문서들의 집합 크기가 작을수록 정확도는 더 높아지고, 커질수록 점점 더 근사치를 계산하게 된다.  
큰 크기의 데이터를 계산할 때는 그만큼 메모리를 많이 사용하게 되기 때문에 메모리를 효과적으로 사용하기 위해 근사치를 사용하는 것이다.

큰사치 계산에서는 정확도와 메모리 사용량 사이의 적절한 균형을 찾아내는 것이 중요한데, 백분위 수 집계에서는 compression 값을 통해 이를 조정할 수 있다.  
compression 값의 크기가 클수록 기본 트리 데이터 구조의 크기가 커지기 때문에 알고리즘이 느려지고 메모리 사용량이 더 높아지지만 더 정확한 결과를 얻을수 있다.
---
**참고**   
TDigest 알고리즘
Tdigest 알고리즘은 백분위 수의 근사치를 계산하기 위해 노드를 사용한다.  
이 노드가 많아질수록 데이터의 양에 비례하는 정확도가 올라가는 것이다.  
앞서 설명했던 compression 값은 최대 노드 수를 제한하기 위한 값이다.  
메모리 사용량 = 20 * 노드크기 * compression  

노드는 약 32바이트 메모리를 사용한다.  
기본 compression 값으로 최악의 경우 64kb 크기의 TDigest를 생성한다.  
이는 최악의 경우에 해당하고 문서별로 해당 필드의 수치가 다양할 것이기 때문에 대부분의 경우 이보다 적은 양의 메모리를 사용할 것이다.
---

### 5.2.10 백분위 수 랭크 집계
백분위 수 랭크 집계 (percentile Ranks Aggregation)는 다중 숫자 메트릭 집계에 해당한다.  
백분위 수와는 반대라고 생각하면 된다.  
백분위 수 집계의 경우 백분위를 지정해서 각 백분위 수에 해당하는 수치를 확인할 수 있었다면,  
백분위 수 랭크의 경우 특정 필드 수치를 통해 백분위의 어느 구간에 속하는지 확인할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "bytes_percentile_ranks": {
      "percentile_ranks": {
        "field": "bytes",
        "values": [
          5000,10000
        ]
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_percentile_ranks": {
      "values": {
        "5000.0": 32.36319460394327,
        "10000.0": 45.062869896300825
      }
    }
  }
}
~~~

### 5.2.11 지형 경계 집계
지형 경계 집계(Geo Bounds Aggregation)는 지형 좌표를 포함하고 있는 필드에 대해 해당 지역 경계상자를 계산하는 메트릭 집계다.  
이 집계를 사용하려면 계산하려는 필드의 타입이 geo_point 여야 한다. 
먼저 다음 질의를 통해 매핑정보를 확인해본다.
~~~
GET /_mappings
~~~
질의를 수행하고 나면 현재 색인된 인덱스에 대한 매핑 정보가 반환되며, 지형정보로 사용할 geoip 필드의 매핑 정보는 다음과 같다
~~~
"geoip": {
          "properties": {
            ... 생략 ...
            "location": {
              "properties": {
                "lat": {
                  "type": "float"
                },
                "lon": {
                  "type": "float"
                }
              }
            }
          }
~~~
geoip 오브젝트 타입 필드 하위에 location 필드가 존재하는데 이 필드가 지형 정보를 가지고 있다.  
하지만 이 필드의 타입은 geo_point 타입이 아닌 오브젝트 타입이다. 즉, 지형과 관련된 집계를 수행할수 없는 필드이기 때문에  
해당 필드의 매핑 정보를 변경하기 위해 재색인 절차가 필요하다.

이를 위해 이 책의 예제에서는 미리 생성해둔 스냅숏을 복원해서 사용한다.  터미널에서 다음명령을 실행한다
~~~
crul -XPOST 'http://localhost:9200/_snapshot/apache-web-log/applied-mapping/_restore'
~~~

정상적으로 복원되면 다음 결과가 반환된다
~~~
{
  "accepted": true
}
~~~
인덱스 정보를 확인해본다. apache-web-log-applied-mapping 인덱스가 생성됐다.
~~~
GET /_cat/indices/apache*?v&pretty

health status index                          uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   apache-web-log-applied-mapping l67UCg1ZRvy7j1eiwhEmMQ   5   1      10001            0      9.7mb          9.7mb
yellow open   apache-web-log                 zkKZfOw6RaiFKPM1iMCbkQ   5   1      10001            0      8.8mb          8.8mb
~~~
매핑 정보를 확인해보면 다음과 같이 geoip.location 필드의 타입이 gio_point로 인식된 것을 확인할 수 있다.
~~~
GET /apache-web-log-applied-mapping/_mapping/field/geoip.location

{
  "apache-web-log-applied-mapping": {
    "mappings": {
      "geoip.location": {
        "full_name": "geoip.location",
        "mapping": {
          "location": {
            "type": "geo_point"
          }
        }
      }
    }
  }
}
~~~
이제 집계를 수행해 본다. 수집된 모든 데이터에 대한 지형 경계를 집계한다.
~~~
GET /apache-web-log-applied-mapping/_search?size=0
{
  "aggs": {
    "viewport": {
      "geo_bounds": {
        "field": "geoip.location",
        "wrap_longitude": true
      }
    }
  }
}
~~~
경계 집계는 수집된 데이터의 범위 중 가장 끝부분에 위치한 정보로 경계가 정해지기 때문에 굉장히 넓은 범위의 위도, 경도 좌표가 지정된다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "viewport": {
      "bounds": {
        "top_left": {
          "lat": 69.34059997089207,
          "lon": -159.76670005358756
        },
        "bottom_right": {
          "lat": -45.88390002027154,
          "lon": 176.91669998690486
        }
      }
    }
  }
}
~~~
위도, 경도 수치만 봐선 번위를 짐작하기 어렵기 때문에 대부분의 경우 다음과 같이 키바나에서 제공하는 차트로 좌표 정보를 시각화해서 제공한다.  

이번에는 범위를 좀 더 좁혀서 유럽 국가에 대해서만 경계를 표시하도록 질의를 변경해보자.
~~~
GET /apache-web-log-applied-mapping/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.continent_code": {
            "query": "EU"
          }
        }
      }
    }
  },
  "aggs": {
    "viewport": {
      "geo_bounds": {
        "field": "geoip.location",
        "wrap_longitude": true
      }
    }
  }
}
~~~
그러면 해당 국가의 사용자들이 접속한 위치의 경계 범위가 반환된다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "viewport": {
      "bounds": {
        "top_left": {
          "lat": 69.34059997089207,
          "lon": -16.358700077980757
        },
        "bottom_right": {
          "lat": 28.534799963235855,
          "lon": 88.22059999220073
        }
      }
    }
  }
}
~~~
마찬가지로 카바나에서 제공하는 차트를 통해 시각화하면 결과를 좀 더 명확히 확인할 수 있다.

### 5.2.12 지형 중심 집계
지형 중심 집계(Geo Centroid Aggregation)는 앞서 살펴본 지형 경계 집계의 범위에서 정가운데의 위치를 반환한다.  
~~~
GET /apache-web-log-applied-mapping/_search?size=0
{
  "aggs": {
    "centroid": {
      "geo_centroid": {
        "field": "geoip.location"
      }
    }
  }
}
~~~
결과 
~~~
{
  ... 생략 ...
  "aggregations": {
    "centroid": {
      "location": {
        "lat": 38.71564010443365,
        "lon": -22.18982591475767
      },
      "count": 9993
    }
  }
}
~~~

## 5.3 버킷 집계
버킷 집계(Bucket Aggregations)는 메트릭 집계와는 다르게 메트릭을 계산하지 않고 버킷을 생성한다.  
생성되는 버킷은 쿼리와 함께 수행되어 쿼리 결과에 따른 컨텍스트 내에서 집계가 이뤄진다.  
이렇게 집계된 버킷은 또 다시 하위에서 집계를 한 번 더 수행해서 집계된 결과에 대해 중첩된 집계를 수행하는 것이 가능하다.

버킷을 생성한다는 것은 집꼐된 결과 데이터 집합을 메모리에 저장한다는 의미이기 때문에 중첩되는 단계가 깊어질수록 메모리 사용량은 점점 더 증가해서 성능에 악영향을 줄 수 있다.  
이러한 문제점 때문에 엘라스틱서치느에서는 기본적으로 사용 가능한 최대 버킷 수가 미리 정의돼 있다.  
그리고 이는 search.max_buckets 값을 변경하는 것으로 조정할 수 있다.

만약 집계 질의를 요청할 때 버킷 크기를 -1 (전체대상) 또는 10000 이상의 값을 지정할 경우에는 엘라스틱서치에서 경고 메세지를 반환한다.  
그러므로 안정적인 집계를 위해서는 성능 측면을 충분하게 고려한 후 집계를 수행해야 한다.

### 5.3.1 범위집계
범위 집계(Range Aggregations)는 사용자가 지정한 범위 내에서 집계를 수행하는 다중 버킷 집계다.  
집계가 수행되면 추출된 문서가 범위에 해당하는지 검증하게 되고, 범위에 해당하는 문서들에 대해서만 집계가 수행된다.  
범위 집계에서는 from과 to 속성을 지정하는데 from을 시작으로 to까지의 범위 내에서 집계가 수행된다.  
이때 to에 지정한 값은 결과에서 제외되므로 주의해야 한다. 즉, from부터 to 미만의 범위로 집계가 수행된다.

~~~
GET /apache-web-log-applied-mapping/_search?size=0
{
  "aggs": {
    "bytes_range": {
      "range": {
        "field": "bytes",
        "ranges": [
          {
            "from": "1000",
            "to": "2000"
          }
        ]
      }
    }
  }
}
~~~
다음과 같이 데이터 크기가 1000~2000바이트(2000은 제외)에 해당하는 문서가 754개인 것을 확인할 수 있다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_range": {
      "buckets": [
        {
1.        "key": "1000.0-2000.0",
2.        "from": 1000.0,
3.        "to": 2000.0,
4.        "doc_count": 754
        }
      ]
    }
  }
}
~~~
1. 집계가 수행될 범위
2. 범위의 시작 값
3. 범위의 긑 값(이 값은 제외)
4. 범위 내의 문서 수

질의 시 range 속성의 타입을 보면 배열 형태인 것을 볼 수 있는데, 짐작할 수 있듯이 여러 개의 범위를 지정해 각각 결과를 반환받을 수 있다.  
~~~
GET /apache-web-log-applied-mapping/_search?size=0
{
  "aggs": {
    "bytes_range": {
      "range": {
        "field": "bytes",
        "ranges": [
          {
            "to": "1000"
          },
          {
            "from": "1000",
            "to": "2000"
          },
          {
            "from": "2000",
            "to": "3000"
          }
        ]
      }
    }
  }
}
~~~
결과는 다음과 같다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_range": {
      "buckets": [
        {
          "key": "*-1000.0",
          "to": 1000.0,
          "doc_count": 666
        },
        {
          "key": "1000.0-2000.0",
          "from": 1000.0,
          "to": 2000.0,
          "doc_count": 754
        },
        {
          "key": "2000.0-3000.0",
          "from": 2000.0,
          "to": 3000.0,
          "doc_count": 81
        }
      ]
    }
  }
}
~~~
결과로 제공되는 key에는 기본적으로 범위가 지정돼 있는데 좀 더 명확하게 표현하기 위해 다음과 같이 직접 원하는 정보를 설정할 수도 있다.
~~~
GET /apache-web-log-applied-mapping/_search?size=0
{
  "aggs": {
    "bytes_range": {
      "range": {
        "field": "bytes",
        "ranges": [
          {
            "key": "small",
            "to": "1000"
          },
          {
            "key": "medium",
            "from": "1000",
            "to": "2000"
          },
          {
            "key": "large",
            "from": "2000",
            "to": "3000"
          }
        ]
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_range": {
      "buckets": [
        {
          "key": "small",
          "to": 1000.0,
          "doc_count": 666
        },
        {
          "key": "medium",
          "from": 1000.0,
          "to": 2000.0,
          "doc_count": 754
        },
        {
          "key": "large",
          "from": 2000.0,
          "to": 3000.0,
          "doc_count": 81
        }
      ]
    }
  }
}
~~~

### 5.3.2 날짜 범위 집계
날짜 범위 집계(Date Range Aggregations)는 범위 집계와 유사하지만 숫자 값을 범위로 사용했던 범위 집계와는 달리 날짜 값을 범위로 집계를 수행한다.  
from 속성에는 시작 날짜 값을 설정하고, to 속성에는 범위의 마지막 날짜 값을 설정한다. 이때 마지막 날짜는 제외된다.

다음은 특정 기간 동안 서버로 전달된 요청 수를 집계한 것이다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "request count with date range": {
      "date_range": {
        "field": "timestamp",
        "ranges": [
          {
            "from": "2015-01-05T05:14:00.000Z",
            "to": "2015-01-05T05:16:00.000Z"
          }
        ]
      }
    }
  }
}
~~~
날짜 형식으로는 엘라스틱서치에서 지원하는 형식만 사용해야 한다. 날짜 형식에 관해서는 공식 메뉴얼을 참고한다.
결과는 다음과 같다
~~~
{
  ... 생략 ...
  "aggregations": {
    "request count with date range": {
      "buckets": [
        {
1.        "key": "2015-01-05T05:14:00.000Z-2015-01-05T05:16:00.000Z",
2.        "from": 1.42043484E12,
3.        "from_as_string": "2015-01-05T05:14:00.000Z",
4.        "to": 1.42043496E12,
5.        "to_as_string": "2015-01-05T05:16:00.000Z",
6.        "doc_count": 0
        }
      ]
    }
  }
}
~~~
1. 집계에 대한 날짜 범위
2. 시작 날짜에 해당하는 밀리초 값
3. 시작 날짜와 문자열 표현
4. 마지막 날짜에 해당하는 밀리초 값
5. 마지막 날짜의 문자열 표현
6. 날짜 범위에 해당하는 문서 수

여러 개의 날짜 범위를 지정하는 방법과 key 값을 사용자 정의하는 방법은 범위 집계와 동일하므로 앞서 설명한 범위 집계를 참고한다.

### 5.3.3 히스토그램 집계
히스토그램 집계(Histogram Aggregations)는 숫자 범위를 처리하기 위한 집계다. 지정한 범위 내에서 집계를 수행하는 범위집계와는 달리 지정한 수치가 간격을 나타내고,  
이 간격의 범위 내에서 집계를 수행한다.

예를들어, 서버로 유입되는 데이터 크기를 나타내는 bytes 필드가 있다고 가정한다면 히스토그램의 간격을 10000 으로 설정한 경우 0~10000바이트(10000은 제외),  
10000~20000바이트(20000은 제외)간격으로 집계를 수행한다.

다음은 히스토그램 간격을 10000으로 설정한 예다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "bytes_histogram": {
      "histogram": {
        "field": "bytes",
        "interval": 10000
      }
    }
  }
}
~~~
그럼 다음과 같이 각 간격에 해당하는 문서의 수를 확인할 수 있다.
~~~
... 생략 ...
"aggregations": {
    "bytes_histogram": {
      "buckets": [
        {
          "key": 0.0,
          "doc_count": 4196
        },
        {
          "key": 10000.0,
          "doc_count": 1930
        },
        {
          "key": 20000.0,
          "doc_count": 539
        },
        {
          "key": 30000.0,
          "doc_count": 596
        },
        {
          "key": 40000.0,
          "doc_count": 175
        },
        ... 생략 ...
       ]
      }
    }
  }
~~~
보다시피 10000을 범위로 하는 문서의 수가 합산된 결과를 확인할 수 있는데 문서 개수가 0인 간격도 포함돼 있다. 만약 문서가 존재하지 않는 구간은 필요하지 않다면  
다음과 같이 최소 문서 수(min_doc_count)를 설정해서 해당 구간을 제외시킬 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "bytes_histogram": {
      "histogram": {
        "field": "bytes",
        "interval": 10000,
        "min_doc_count": 1
      }
    }
  }
}
~~~
구간별로 최소한 1개 이상의 문서를 포함한 경우에만 결과를 반환하도록 설정했기 때문에 문서가 존재하지 않은 구간은 제외되어 결과로 반환되지 않는다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_histogram": {
      "buckets": [
        {
          "key": 0.0,
          "doc_count": 4196
        },
        {
          "key": 10000.0,
          "doc_count": 1930
        },
        {
          "key": 20000.0,
          "doc_count": 539
        },
        {
          "key": 30000.0,
          "doc_count": 596
        },
        {
          "key": 40000.0,
          "doc_count": 175
        },
        ... 생략 ...
      ]
    }
  }
}
~~~

### 5.3.4 날짜 히스토그램 집계
날짜 히스토그램 집계(Date Histogram Aggregation)는 다중 버킷 집계에 속하며 히스토그램 집계와 유사하다.  
히스토그램 집계는 숫자 값을 간격으로 삼아 구간별 집계를 수행한 반면 날짜 히스토그램 집계는 분,시간,연도를 구간으로 집계를 수행할 수 있다.  
분 단위로 얼마큼의 사용자 유입이 있었는지 확인해 보기 위해 다음과 같은 집계를 수행해 보자.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "daily_request_count": {
      "date_histogram": {
        "field": "timestamp",
        "calendar_interval": "minute"
      }
    }
  }
}
~~~
다음과 같이 분단위로 문서 수를 합산한 결과가 반환된다.
~~~
"aggregations": {
    "daily_request_count": {
      "buckets": [
        {
1.        "key_as_string": "2015-05-17T10:05:00.000Z",
2.        "key": 1431857100000,
3.        "doc_count": 74
        },
        {
          "key_as_string": "2015-05-17T10:06:00.000Z",
          "key": 1431857160000,
          "doc_count": 0
        },
        {
          "key_as_string": "2015-05-17T10:07:00.000Z",
          "key": 1431857220000,
          "doc_count": 0
        },
~~~
1. 지정한 interval 값에 따른 구간 시작 일자. UTC가 기본이며 "yyyy-MM-dd'T'HH:mm:ss.SSS" 형식을 사용한다.
2. 1의 날짜에 해당하는 밀리초 값
3. 해당 구간의 문서 수

구간을 지정하기위해 calendar_interval 속성을 사용했는데, 여기에 year, quarter, month, week, day, hour, minute, second 표현식을 사용할 수 있다.  
더 세밀한 설정을 하기 위해 30m(30분 간격), 1.5h(1시간 30분 간격) 같은 값으로도 설정할 수 있다.

더 간단한 날짜 형식("yyyy-MM-dd")으로 변환해보자.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "daily_request_count": {
      "date_histogram": {
        "field": "timestamp",
        "calendar_interval": "day",
        "format": "yyyy-MM-dd"
      }
    }
  }
}
~~~
그러면 다음과 같이 이전에 "2015-05-17T10:07:000Z"였던 날짜 형식이 "2015-05-17"로 변경된 것을 확인할 수 있다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "daily_request_count": {
      "buckets": [
        {
          "key_as_string": "2015-05-17",
          "key": 1431820800000,
          "doc_count": 1632
        },
        {
          "key_as_string": "2015-05-18",
          "key": 1431907200000,
          "doc_count": 2893
        },
        {
          "key_as_string": "2015-05-19",
          "key": 1431993600000,
          "doc_count": 2896
        },
        {
          "key_as_string": "2015-05-20",
          "key": 1432080000000,
          "doc_count": 2578
        }
      ]
    }
  }
}
~~~
지금까지 사용한 날짜 데이터는 모두 UTC 기준으로 기록돼 있다.   
우리나라 사용자가 UTC값을 반환받아 사용할 경우에는 항상 여기에 9시간을 더해서 계산해야 현재 시간이 되기 때문에 매우 번거로울 수 있다.  
이를 위해 엘라스틱서치에서는 타임존(Time Zone)을 지원한다.  
쿼리를 수행할 때 타임존을 설정하면 날짜 값이 UTC가 아닌 한국 시간으로 반환된 결과를 받을 수 있다.  
그럼 타임존을 이용해 UTC를 한국시간으로 변경해보자. 한국 시간의 경우 UTC에 9시간을 더해야 하므로 타임존을 "+09:00"로 설정해야한다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "daily_request_count": {
      "date_histogram": {
        "field": "timestamp",
        "calendar_interval": "day",
        "time_zone": "+09:00"
      }
    }
  }
}
~~~
UTC보다 날짜가 더 이전인 경우에는 "-09:00"과 같이 + 대신 - 를 사용해서 지정하면 된다.  
~~~
{
  ... 생략 ...
  "aggregations": {
    "daily_request_count": {
      "buckets": [
        {
          "key_as_string": "2015-05-17T00:00:00.000+09:00",
          "key": 1431788400000,
          "doc_count": 538
        },
        {
          "key_as_string": "2015-05-18T00:00:00.000+09:00",
          "key": 1431874800000,
          "doc_count": 2898
        },
        {
          "key_as_string": "2015-05-19T00:00:00.000+09:00",
          "key": 1431961200000,
          "doc_count": 2902
        },
        {
          "key_as_string": "2015-05-20T00:00:00.000+09:00",
          "key": 1432047600000,
          "doc_count": 2862
        },
        {
          "key_as_string": "2015-05-21T00:00:00.000+09:00",
          "key": 1432134000000,
          "doc_count": 799
        }
      ]
    }
  }
}
~~~
타임존과 다르게 offset을 사용하면 집계 기준이 되는 날짜 값의 시작 일자를 조정할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "daily_request_count": {
      "date_histogram": {
        "field": "timestamp",
        "calendar_interval": "day",
        "offset": "+3h"
      }
    }
  }
}
~~~
이와같이 기존 날짜값에 3시간이 더해져서 집계가 수행된 것을 확인할 수 있다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "daily_request_count": {
      "buckets": [
        {
          "key_as_string": "2015-05-17T03:00:00.000Z",
          "key": 1431831600000,
          "doc_count": 1991
        },
        {
          "key_as_string": "2015-05-18T03:00:00.000Z",
          "key": 1431918000000,
          "doc_count": 2898
        },
        {
          "key_as_string": "2015-05-19T03:00:00.000Z",
          "key": 1432004400000,
          "doc_count": 2895
        },
        {
          "key_as_string": "2015-05-20T03:00:00.000Z",
          "key": 1432090800000,
          "doc_count": 2215
        }
      ]
    }
  }
}
~~~

### 5.3.5 텀즈 집계
텀즈 집계(Terms Aggregations)는 버킷이 동적으로 생성되는 다중 버킷 집계다.  
집계 시 지정한 필드에 대해 빈도수가 높은 텀의 순위로 결과가 반환된다. 이를 통해 가장 많이 접속하는 사용자를 알아낸다거나 국가별로  
어느 정도의 빈도로 서버에 접속하는지 등의 집계를 수행할 수 있다.  
텀즈 집계를 통해 아파치 서버로 얼마만큼의 요청이 들어왔는지를 국가별로 집계해보자.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "request count by country": {
      "terms": {
        "field": "geoip.country_name.keyword"
      }
    }
  }
}
~~~
country_name 필드는 Text 데이터 타입과 Keyword 데이터 타입을 모두 가지는 멀티 필드로 구성돼 있는데 집계할 때는 반드시 Keyword 데이터 타입의  
필드를 사용해야 한다. Text 데이터 타입의 경우에는 형태소 분석기를 통해 분석하는 과정이 항상 동반되기 때문에 집계할 때는 형태소 분석이 필요 없는 Keyword 데이터 타입을 사용해야만 한다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "request count by country": {
1.    "doc_count_error_upper_bound": 47,
2.    "sum_other_doc_count": 2334,
3.    "buckets": [
        {
4.        "key": "United States",
5.        "doc_count": 3974
        },
        {
          "key": "France",
          "doc_count": 855
        },
        {
          "key": "Germany",
          "doc_count": 510
        },
        {
          "key": "Sweden",
          "doc_count": 440
        },
        {
          "key": "India",
          "doc_count": 428
        },
        ... 생략 ...
      ]
    }
  }
}
~~~
1. 문서 수에 대한 오류의 상한선(각 샤드별로 계산되는 집계의 성능을 고려해 근사치를 계산하기 때문에 문서 수가 정확하지 않을 수 있다.)
2. 결과에 포함되지 않은 모든 문서 수
3. 최상위 버킷 목록(집계된 결과에서 상위 결과부터 설정한 size만큼을 반환. 기본값 10)
4. 질의 시 지정한 필드에 해당하는 값(여기서는 geoip.country_name)
5. 해당 필드 값과 동일한 문서의 수(여기서는 같은 나라 이름을 가진 문서의 수)

텀즈 집계결과를 살펴보면 sum_other_doc_count가 2334인 것을 확인할 수 있다.  
이는 반환 된 결과에 포함되지 않은 집계 결과가 남아있다는 것을 의미하기 때문에 size 속성의 기본값인 10개 보다 더 많은 결과를 반환받기 위해서는 size 값을 지정해야 한다.

집계를 수행할 때는 각 샤드에 집계 요청을 전달하고, 각 샤드는 집계 결과에 대해 정렬을 수행하는 자체 뷰를 갖게 된다.  
이것들을 병합해서 최종 뷰를 만들기 때문에 포함되지 않은 문서가 존재하는 경우에는 집계 결과가 정확하지 않을 수 있다.

예를들어, 상품에 대한 수를 집계한다고 가정하고, 각 샤드는 다음과 같이 데이터를 색인하고 있다고 해보자.

|  -  |     샤드 A      |     샤드 B      |     샤드 C      |
|:---:|:-------------:|:-------------:|:-------------:|
|  1  | Product A(25) | Product A(30) | Product A(45) |
|  2  | Product B(18) | Product B(25) | Product C(44) |
|  3  | Product C(6)  | Product F(17) | Product Z(36) |
|  4  | Product D(3)  | Product Z(16) | Product G(30) |
|  5  | Product E(2)  | Product G(15) | Product E(29) |
|  6  | Product F(2)  | Product H(14) | Product H(28) |
|  7  | Product G(2)  | Product I(10) | Product Q(2)  |
|  8  | Product H(2)  | Product Q(6)  | Product D(1)  |
|  9  | Product I(1)  | Product J(8)  |               |
| 10  | Product J(1)  | Product C(4)  |               |

여기서 집계 시 size 값을 5로 지정했다면 다음과 같이 각 샤드별로 상위 5개의 집계 결과를 반환하게 될 것이다.

|  -  |     샤드 A      |     샤드 B      |     샤드 C      |
|:---:|:-------------:|:-------------:|:-------------:|
|  1  | Product A(25) | Product A(30) | Product A(45) |
|  2  | Product B(18) | Product B(25) | Product C(44) |
|  3  | Product C(6)  | Product F(17) | Product Z(36) |
|  4  | Product D(3)  | Product Z(16) | Product G(30) |
|  5  | Product E(2)  | Product G(15) | Product E(29) |

위 결과를 보면 상품 A 의 경우에는 A, B, C 샤드에서 전부 결과를 합산했기 때문에 정확한 값이라고 볼 수 있지만 상품 C 의 경우에는 샤드 B에 포함돼 있지 않기 때문에  
저확한 값이 아니다. 각 샤드의 뷰를 병합해서 최종 뷰를 생성한 결과를 확인해 보자.

|  -  |       -        |
|:---:|:--------------:|
|  1  | Product A(100) |
|  2  | Product Z(52)  |
|  3  | Product C(50)  |
|  4  | Product G(45)  |
|  5  | Product B(43)  |

3개의 샤드에서 상품 C를 정확하게 집계했다면 50이 아닌 54가 결과로 반환됐을 것이다.  
그러면 상품 Z보다 값이 크기 때문에 2순위로 올라가야 하지만 결과는 그렇지 못하다.  
이처럼 집계 시 모든 문서가 포함되지 않은 경우에는 정확하지 않은 결과가 반환 될 수 있음에 주의해야 한다.

다시 돌아와 앞의 예제에서 아직 포함되지 않은 문서의 수가 2개였기 때문에 모든 문서를 포함할 수 있도록 size 값을 100으로 지정한 후 질의를 수행해본다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "request count by country": {
      "terms": {
        "field": "geoip.country_name.keyword",
        "size": 100
      }
    }
  }
}
~~~

결과를 확인해보면 포함되지 않은 문서 없이 모든 문서에 대해 집계 결과가 반환된 것을 확인할 수 있다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "request count by country": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": "United States",
          "doc_count": 3974
        },
        {
          "key": "France",
          "doc_count": 855
        },
        {
          "key": "Germany",
          "doc_count": 510
        },
        {
          "key": "Sweden",
          "doc_count": 440
        },
        {
          "key": "India",
          "doc_count": 428
        },
        {
          "key": "China",
          "doc_count": 416
        },
        ... 생략 ...
      ]
    }
  }
}
~~~
이처럼 집계에 포함되지 않은 문서들을 포함시키기 위해 size 값을 늘리면 그만큼 집계의 정확도가 높아진다. 하지만 버킷에 더 많은 양의 데이터를 담아야 하기 때문에  
메모리 사용량과 결과를 계산하는 데 드는 처리 비용 또한 증가한다.
---
**참고**  
집계화 샤드 크기  

텀즈 집계가 수행될 때 검색 프로세스를 관장하는 노드에서는 각 샤드에게 최상위 버킷을 제동하도록 요청한 후에 모든 샤드로부터 결과를 받을 때까지 기다린다.  
결과를 기다리다가 모든 샤드로부터 결과를 받으면 설정된 size에 맞춰 하나로 병합한 후 클라이언트에 결과로 전달한다.  
각 샤드에는 정확성을 위해 size의 크기만큼이 아닌 샤드 크기를 이용한 경험적인 방법(t샤드 크기 * 1.5 +10)을 사용해 내부적으로 집계를 수행하는데,  
텀즈 집계의 결과로 받을 텀의 개수를 정확하게 파악할 수 있는 경우에는 shard_size 속성을 사용해 각 샤드에서 집계할 크기를 직접 지정해 불필요한 연산을 줄이면서도 정확도를 높일 수 있다.  
앞서 설명한 바와 같이 shard_size가 기본값(-1)로 설정된 경우에는 엘라스틱서치가 샤드 크기를 기준으로 자동으로 추정한다.   
만약 shard_size를 직접 설정할 경우에는 size보다 작은 값은 설정할 수 없음에 주의해야 한다.
---

집계 결과를 다시 살펴보면 반환된 결과중 doc_count_error_upper_bound라는 값이 있다.  
이것은 최종 집계 결과에서 포함되지 않은 잠재 문서의 수를 의미한다.  
이 수치는 각 샤드별로 반환된 집계 결과 중 최종 병합 과정에서 선택받지 못한 집계 결과의 가장 마지막 값을 합산한 결과다.  

앞에서 정확성을 이야기하며 예로 들었던 상품 테이블을 살펴보면 각 샤드에서 선택되지 못한 상품 집계중 맨 마지막 값은 다음과 같다.

|  -  |     샤드 A      |     샤드 B      |     샤드 C      |
|:---:|:-------------:|:-------------:|:-------------:|
|  5  | Product E(2) | Product G(15) | Product E(29) |

이 상품들의 값을 모두 더하면 46이 되고, 이는 최악의 경우 현재 집계된 결과보다 상위에 집계될 수 있는 문서가 존재할 수도 있음을 의미한다.  
이러한 경우에는 size 또는 shard_size를 조절해서 정확도를 높이 것이 좋다.  
하지만 정확도와 속도 사이에는 항상 트레이드오프가 있다는 점을 명심 해야 한다.

## 5.4 파이프라인 집계
파이프라인 집계(Pipeline Aggregations)는 다른 집계와 달리 쿼리 조건에 부합하는 문서에 대해 집계를 수행하는 것이 아니라 다른집계로 생성된 버킷을 참조해 집계를 수행한다.  
집계 또는 중첩된 집계를 통해 생성된 버킷을 사용해 추가적으로 계산을 수행한다고 보면 된다.
파이프라인 집계에는 부모(Parent), 형제(Sibling)라는 두 가지 유형이 있다.  

파이프라인 집계를 수행할 때는 buckets_path 파라미터를 사용해 참조할 집계의 경로를 지정함으로써 체인 형식으로 집계 간의 연산이 이뤄진다.  
파이프라인 집계는 모든 집계가 완료된 후에 생성된 버킷을 사용하기 때문에 하위 집계를 가질 수는 없지만 다른 파이프라인 집계와는 buckets_path를 통해 참조하도록 지정할 수 있다.
즉 다른 집계를 통해 생성된 버킷을 사용해 또 다른 집계를 수행한다.

집계를 참조하는 방법은 다음과 같다.
~~~
AGG_SEPARATOR = '>' ;
METRIC_SEPARATOR = '.' ;
AGG_NAME = <집계 이름> ;
METRIC = <메트릭 집계 이름(다중 메트릭 집계인 경우)> ;
PATH = <AGG_NAME> [<AGG_SEPARATOR>, <AGG_NAME>] * [<METRIC_SEPARATOR>, <METRIC>]
~~~

### 5.4.1 형제 집계
형제집계(Sibling Aggregation)는 동일 선상의 위치에서 수행되는 새 집계를 의미한다. 즉, 형제 집계를 통해 수행되는 집계는 기존 버킷에 추가되는 형태가 아니라  
동일 선상의 위치에 새 집계가 생성되는 파이프라인 집계다.  
형제집계에는 다음과 같은 집계가 포함된다.
- 평균 버킷 집계(Avg Bucket Aggregation)
- 최대 버킷 집계(Max Bucket Aggregation)
- 최소 버킷 집계(Min Bucket Aggregation)
- 합계 버킷 집계(Sum Bucket Aggregation)
- 통계 버킷 집계(Stats Bucket Aggregation)
- 확장 통계 버킷 집계(Extended Bucket Aggregation)
- 백분위수 버킷 집계(Percentiles Bucket Aggregation)
- 이동 평균 집계(Moving Bucket Aggregation)

아파치 웹 로그에서 분 단위로 합산된 데이터량 중 가장 큰 데이터량을 구하고 싶은경우, 기존방법으로는 date_histogram과 그 하위 집계로 sum 집계를 수행한 후  
가장 큰 값을 추려내야 한다.  

파이프라인 집계 중 최대 버킷 집계를 통해 분 단위 데이터량 합산과 더불어 가장 큰 데이터량을 구해보자.
~~~
GET /apache-web-log/_search?size=0
{
 "aggs": {
   "histo": {
1.   "date_histogram": {
       "field": "timestamp",
       "calendar_interval": "minute"
     },
2.   "aggs": {
       "bytes_sum": {
         "sum": {
           "field": "bytes"
         }
       }
     }
   },
3. "max_bytes": {
4.   "max_bucket": {
5.     "buckets_path": "histo>bytes_sum"
     }
   }
 }
}
~~~
1. 아파치 웹 로그를 단위로 집계
2. 분 단위로 데이터량의 합 집계
3. 파이프라인 집계의 이름
4. 합산된 데이터량 가운데 가장 큰 값을 구하기 위해 max_bucket 사용
5. 참조할 버킷으로 histo 버킷의 bytes_sum 버킷 참조

위 예제에서 buckets_path에 histo>bytes_sum을 지정해 가장 큰 값을 구할 수 있었다.  
histo는 가장 상위 집계인 date_histogram의 이름이고 bytes_sum은 그 하위 집계인 sum 집계의 이름이다.  
여기서 sum 집계는 결과가 하나만 존재하는 단일 메트릭 집계이기 때문에 집계 이름만으로 참조할 수 있지만 stats 같은 다중 메트릭 집계의 경우에는 메트릭명까지 지정해야 한다.

예를 들어, bytes_stats이라는 이름의 stats 집계에서 평균 값을 참조하려는 경우 buckets_path에 histo>bytes_stats.avg라고 지정해야 한다.
앞 예제의 결과를 살펴보면 다음과 같다.
~~~
... 생략 ...
  "aggregations": {
    "histo": {
      "buckets": [
        {
          "key_as_string": "2015-05-17T10:05:00.000Z",
          "key": 1431857100000,
          "doc_count": 74,
          "bytes_sum": {
            "value": 5185322.0
          }
        },
        {
          "key_as_string": "2015-05-17T10:06:00.000Z",
          "key": 1431857160000,
          "doc_count": 0,
          "bytes_sum": {
            "value": 0.0
          }
        },
        {
          "key_as_string": "2015-05-17T10:07:00.000Z",
          "key": 1431857220000,
          "doc_count": 0,
          "bytes_sum": {
            "value": 0.0
          }
        },
        ... 생략 ...
        "max_bytes": {
      "value": 2.06109322E8,
      "keys": [
        "2015-05-18T21:05:00.000Z"
      ]
    }
  }
}
~~~
date_histogram 의 집계명인 histo의 하위로 집계 결과 버킷이 생성됐고, 이와 동일한 위치에 max_bytes라는 최대 버킷 집계의 결과가 추가됐다.  
이처럼 동일한 위치에 새로운 집계 결과가 추가되는 것이 형제 집계다.  
이 밖에도 형제 집계에는 최대/최소/평균/통계/확장통계/백분위수/이동평균 버킷 집계까 있으며, 다음과같이 수행할 수 있다.  
~~~
//최대 버킷 집계
{
  "max_bucket": {
     "buckets_path": "histo>bytes_sum"
  }
}
 
 //최소 버킷 집계
{
  "min_bucket": {
     "buckets_path": "histo>bytes_sum"
  }
}
 
 //평균 버킷 집계
{
  "avg_bucket": {
     "buckets_path": "histo>bytes_sum"
  }
}

//통계 버킷 집계
{
  "stats_bucket": {
     "buckets_path": "histo>bytes_sum"
  }
}

//확장 버킷 집계
{
  "extended_stats_bucket": {
     "buckets_path": "histo>bytes_sum"
  }
}

//백분위수 버킷 집계
{
  "percentiles_bucket": {
     "buckets_path": "histo>bytes_sum"
  }
}

//이동 평균 버킷 집계
{
  "moving_avg_bucket": {
     "buckets_path": "histo>bytes_sum"
  }
}
~~~
파이프라인 집계는 이처럼 집계의 이름을 통해 버킷을 참조할 수 있고, 추가로 다양한 집계를 수행할 수 있게 해준다.

### 5.4.2 부모 집계
부모 집계(Parent Aggregation)는 집계를 통해 생성된 버킷을 사용해 계산을 수행하고, 그 결과를 기존 집계에 반영한다.  
부모 집계에 해당하는 집계는 다음과 같다.
- 파생 집계(Derivative Aggregation)
- 누적 집계(Cumulative Sum Aggregation)
- 버킷 스크립트 집계(Bucket Script Aggregation)
- 버킷 셀렉터 집계(Bucket Selector Aggregation)
- 시계열 차분 집계(Serial Differencing Aggregation)

아파치 웹 로그를 통해 수집된 데이터가 시간이 지남에 따라 변화하는 값의 변경폭 추이를 확인하고 싶은 경우 파생 집계를 활용할 수 있다.  
파생 집계는 부모 히스토그램 또는 날짜 히스토그램 집계에서 지정된 메트릭의 파생 값을 계산하는 상위 파이프라인 집계다.  
이는 부모 히스토그램 집계의 측정 항목에 대해 동작하고, 히스토그램 집계에 의한 각 버킷의 집계 값을 비교해서 차이를 계산한다.  
지정된 메트릭은은 숫자여야 하고, 상위에 해당하는 집계(부모 히스토그램)의 min_doc_count가 0보다 큰 값으로 설정되는 경우  
일부 간격이 결과에서 생략될 수가 있기 때문에 min_doc_count값을 0으로 설정해야 한다.

파생 집계의 경우네는 이처럼 선행되는 데이터가 존재하지 않으면 집계를 수행할 수가 없는데, 실제 데이터를 다루다 보면 종종 노이즈가 포함되기도 하고,  
필요한 필드에 값이 존재하지 않을수도 있다. 이러한 부분을 갭(gap)이라고 할 수 있는데, 쉽게 말해 데이터가 존재하지 않는 부분을 의미한다.  갭은 여러가지 이유로 발생할수 있으며,  
일반적인 이유는 다음과 같다.
- 어느 하나의 버킷 안으로 포함되는 문서들에 요청된 필드가 포함되지 않은 경우
- 하나 이상의 버킷에 대한 쿼리와 일치하는 문서가 존재하지 않는 경우
- 다른 종속된 버킷에 값이 누락되어 계산된 메트릭이 값을 생성할 수가 없는 경우

이러한 경우에는 파이프라인 집계에 원하는 동작을 알리는 메커니즘이 필요하다.  
이러한 역할을 하는 것이 갭 정책(gap_policy)이다. 모든 파이프라인 집계에서는 gap_policy 파라미터를 허용한다.  
현재 두 가지 갭 정책을 선택할 수 있다.
- skip: 이 옵션은 누락된 데이터를 버킷이 존재하지 않는것으로 간주한다. 버킷을 건너뛰고 다음으로 사용 가능한 값을 사용해 계산을 계속해서 수행한다.
- insert_zeros: 이 옵션은 누락된 값을 0으로 대체하며 파이프라인 집계 계산은 정상적으로 진행된다.

파생 집계에 대해 알아봤으니 다음과 같이 집계를 수행해 보자.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
1.  "histo": {
      "date_histogram": {
        "field": "timestamp",
        "calendar_interval": "day"
      },
      "aggs": {
2.      "bytes_sum": {
          "sum": {
            "field": "bytes"
          }
        },
3.      "sum_deriv": {
          "derivative": {
            "buckets_path": "bytes_sum"
          }
        }
      }
    }
  }
}
~~~
1. 부모 히스토그램. 일단위로 집계를 수행한다.
2. 일 단위로 데이터의 크기를 합산하는 sum 집계를 수행한다.
3. 일 단위로 데이터 크기의 합산된 값을 버킷의 현재 값과 이전 값을 비교하는 집계를 수행한다.

결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "histo": {
      "buckets": [
        {
          "key_as_string": "2015-05-17T00:00:00.000Z",
          "key": 1431820800000,
          "doc_count": 1632,
          "bytes_sum": {
            "value": 4.14259902E8
1.        }
        },
        {
          "key_as_string": "2015-05-18T00:00:00.000Z",
          "key": 1431907200000,
          "doc_count": 2893,
          "bytes_sum": {
            "value": 7.88636158E8
          },
          "sum_deriv": {
2.          "value": 3.74376256E8
          }
        },
        {
          "key_as_string": "2015-05-19T00:00:00.000Z",
          "key": 1431993600000,
          "doc_count": 2896,
          "bytes_sum": {
            "value": 6.65827339E8
          },
          "sum_deriv": {
            "value": -1.22808819E8
          }
        },
        {
          "key_as_string": "2015-05-20T00:00:00.000Z",
          "key": 1432080000000,
          "doc_count": 2578,
          "bytes_sum": {
            "value": 8.78559106E8
          },
          "sum_deriv": {
            "value": 2.12731767E8
          }
        }
      ]
    }
  }
}
~~~
1. 버킷의 첫 번째 집계 결과에는 비교할 이전 데이터가 존재하지 않기 때문에 파생 집계 결과가 반환되지 않았다.
2. 버킷의 이전 집계 값인 4.14259902E8와 현재 값인 7.88636158E8을 비교해서 3.74376256E8이라는 집계 결과가 반환됐다.

이처럼 부모 집계는 부모에 해당하는 상위 집계를 통해 생성된 버킷에 대해 집계를 수행한다.  
부모 집계에 해당하는 집계로는 앞서 언급했던 것처럼 파생/누적/버킷스크립트/버킷셀렉터/시계열 차분 집계가 있다.
~~~
//파생 집계
{
  "derivative": {
    "buckets_path": "bytes_sum"
  }
}

//누적 집계
{
  "cumulative_sum": {
    "buckets_path": "bytes_sum"
  }
}

//버킷 스크립트 집계
{
  "bucket_script": {
    "buckets_path": {
      "my_var1": "bytes_sum",
      "my_var2": "total_count"
    },
    "script": "params.my_var1 / params.my_var2"
  }
}

//버킷 셀렉터
{
  "bucket_selector": {
    "buckets_path": {
      "my_var1": "bytes_sum",
      "my_var2": "total_count"
    },
    "script": "params.my_var1 > params.my_var2"
  }
}

//시계열 차분 집계
{₩
  "serial_diff": {
    "buckets_path": "bytes_sum",
    "lag": "7"
  }
}
~~~

## 5.5 근사값으로 제공되는 집계 연산
엘라스틱서치는 다양한종류의 집계연산을 제공하고, 대부분의 경우 전체 데이터를 대상으로 집계가 일어나기 때문에 결과도 매우 정확하게 제공된다.  
하지만 일부 집계 연산의 경우에는 성능 문제로 근삿값(Approximate Count)을 기반으로 한 결과를 제공한다.  
떄문에 어떠한 집계 연산이 근삿값을 제공하는지 명확히 알지 못하면 자칫 큰 낭패를 볼 수도 있다.

### 5.5.1 집계 연산과 정확도
집계를 기능별로 살펴보면 다음과 같이 크게 4가지 종류로 분류할 수 있다.
- 버킷 집계  
특정 기준을 충족하는 문서들을 버킷으로 분류하는 집계
- 메트릭 집계  
버킷에 존재하는 문서들을 이용해 각종 통계 지표를 생성하는 집계
- 파이프라인 집계  
서로 다른 메트릭 집계의 출력을 연결하는 집계 패밀리
- 행렬 집계  
추출한 값을 기반으로 결과를 행렬로 생성하는 집계 패밀리

각 분류 아래에는 다수의 집계 연산이 있는데 이 가운데 실제로 수학적인 계산을 수행하는 것은 메트릭 집계뿐이다.  
메트릭 집계에 속한 집계 연산들은 특정 버킷으로 분류된 문서들을 기반으로 각종 수학적인 계산을 수행하고, 그 결과를 각종 통계 지표로 제공한다.  
메트릭 집계로 제공디는 집계 연산들을 계산 목적에 따라 세 가지 종류로 한 번 더 분류할 수 있다.

1. 일반적인 계산을 위한 집계 연산
   1. 평균 집계
      1. 문서에서 추출한 숫자값의 평균을 계산하는 메트릭 집계
      2. 키워드: avg
   2. 카디널리티 집계
      1. 문서의 특정 필드에 대한 Unique(Distinct) 연산을 근삿값으로 계산하는 메트릭 집계
      2. 성능 문제로 내부적으로 HyperLogLog++ 알고리즘을 이용해 근삿값으로 계산된다(95% 수준)
      3. 정확도를 위해 precision_threshold 옵션을 제공한다.
      4. 키워드: cardinality
   3. 최댓값 집계
      1. 문서에서 추출한 숫자값의 최댓값을 계산하는 메트릭 집계
      2. 키워드: max
   4. 최솟값 집계
      1. 문서에서 추출한 숫자값의 최솟값을 계산하는 메트릭 집계
      2. 키워드: min
   5. 합산 집계
      1. 문서에서 추출한 숫자값의 합계를 계산하는 메트릭 집계
      2. 키워드: sum
2. 고차원 계산을 위한 집계 연산
   1. 지형 경계 집계
   2. 지형 중심 집계
   3. 백분위 수 집계
   4. 백분위 수 랭크 집계
3. 특수한 목적을 위한 집계 연산
## 5.1 집계

### 5.1.1 엘라스틱서치와 데이터 분석
일반적인 통게 프로그램은 배치 방식으로 데이터를 처리한다. 대용량 데이터를 하둡이나 관계형 데이터베이스에 적재하고 배치로 처리하는 식이다.  
반면 엘라스틱 서치는 많은 양의 데이터를 조각내어 관리한다. 덕분에 문서의 수가 늘어나도 배치 처리보다 좀 더 실시간에 가깝게 문서를 처리할 수 있다.  

SQL로는 다음과같이 GROUP BY 구문을 이용해 집계할 수 있다.
~~~
SELECT SUM(ratings) FROM movie_review GROUP BY movie_no;
~~~
엘라스틱 서치의 Query DSL로 집계하는 쿼리는 다음과 같다.
~~~
POST /movie_search/_search
{
  "aggs": {
    "movie_no_agg": {
      "terms": {
        "field": "movie_no"
      },
      "aggs": {
        "ratings_agg": {
          "sum": {
            "field": "ratings"
          }
        }
      }
    }
  }
}
~~~
엘라스틱서치는 SQL보다 더욱 강력한 집계 기능을 제공한다.  
집계를 여러 개 중첩해 사용할 수 있을뿐더러 범위, 날짜, 위치 정보도 집꼐할 수 있다.  
또한 엘라스틱서치는 인덱스를 활용해 분산 처리가 가능하기 때문에 SQL보다 더 많은 데이터를 빠르게 집계할 수 있다.

### 5.1.2 엘라스틱서치가 집계에 사용하는 기술
#### 캐시  
집계 쿼리로 값을 조회하면, 여러 노드에 있는 데이터를 집계해 질의에 답하기 때문에 데이터의 양이 클수록 CPU와 메모리 자원이 많이 소모된다.  
질의에 응답하는 시간 또한 길어진다.  
이런 문제는 노드의 하드웨어 성능을 높이면 해결할 수 있지만 하드웨어 업그레이드는 막대한 비용이 들기 때문에 올바른 해결책이 아니다.  
가장 현실적이고 비용 효율적인 결책은 '캐시'를 이용하는 것이다.  
캐시는 질의결과를 임시 버퍼(캐시)에 둔다. 이후 처리해야 하는 같은 질의에 대해 매번 결과를 계산하는 게 아니라 버퍼에 보관된 결과를 반환한다.  
캐시를 적용하는 것만으로도 인덱스의 성능을 대폭 향상시킬 수 있다.  
캐시의 크기는 일반적으로 힙 메모리의 1%를 할당하며, 캐시에 없는 질의의 경우 성능 향상에 별다른 도움이 되지 않는다.  

캐시는 엘라스틱서치의 conf 폴더 안의 elasticsearch.yml 파일을 수정해 활성화할 수 있다.  
다음은 힙메모리의 2%를 캐시에 할당하는 예다.
~~~
indices.requests.cache.size: 2%
~~~

- Node query Cache  
노드의 모든 샤드가 공유하는 LRU(Least-Recently-Used) 캐시다. 캐시 용량이 가득차면 사용량이 가장 적은 데이터를 삭제하고 새로운 결괏값을 캐싱한다.  
기본적으로 10%의 필터 캐시가 매모리를 제어하고 쿼리 캐싱을 사용할지 여부는 elasticsearch.yml 파일에 옵션으로 추가하면 된다. 기본값은 true
~~~
index.queries.cache.enabled:true
~~~

- Shard request Cache  
엘라스틱서치는 인덱스의 수평적 확산과 병렬 처리를 통한 성능 향상을 위해 고안된 개념이다.  
샤드는 데이터를 분산 저장하기 위한 단위로서, 그 자체가 온전한 기능을 가진 독립 인덱스라고 할 수 있다.  
- Shard request Cache는 바로 이 샤드에서 수행된 쿼리의 결과를 캐싱한다.   
샤드의 내용이 변경되면 캐시가 삭제되기 때문에 업데이트가 빈번한 인덱스에서는 오히려 성능 저하를 일으킬 수 있다.

- Field data Cache  
엘라스틱서치가 필드에서 집계 연산을 수행할 때는 모든 필드 값을 메모리에 로드한다.  
이러한 이유로 엘라스틱서치에서 계산되는 집계 쿼리는 성능적인 측면에서 비용이 많이든다.  
Field data Cache는 집계가 계산되는 동안 필드의 값을 메모리에 보관한다.

### 5.1.3 실습 데이터 살펴보기
스냅숏의 목록을 확인해본다.
~~~
curl -XGET 'http://localhost:9200/_snapshot/apache-web-log/_all?pretty'
~~~
apache-web-log 스냅숏 그룹 내부에는 다음과 같이 2개의 스냅숏이 존재한다.
~~~
{
  "snapshots": [
    {
      "snapshot": "default",
      "uuid": "yzmzEx6uSMS55j60z4buBA",
      "repository": "apache-web-log",
      "version_id": 6040399,
      "version": "6.4.3",
      "indices": [
        "apache-web-log"
      ],
      "data_streams": [],
      "include_global_state": false,
      "state": "SUCCESS",
      "start_time": "2019-03-23T16:03:50.351Z",
      "start_time_in_millis": 1553357030351,
      "end_time": "2019-03-23T16:03:50.604Z",
      "end_time_in_millis": 1553357030604,
      "duration_in_millis": 253,
      "failures": [],
      "shards": {
        "total": 5,
        "failed": 0,
        "successful": 5
      },
      "feature_states": []
    },
    {
      "snapshot": "applied-mapping",
      "uuid": "SgXhqApiSHiauC6fbjSHMw",
      "repository": "apache-web-log",
      "version_id": 6040399,
      "version": "6.4.3",
      "indices": [
        "apache-web-log-applied-mapping"
      ],
      "data_streams": [],
      "include_global_state": false,
      "state": "SUCCESS",
      "start_time": "2019-03-23T16:05:46.038Z",
      "start_time_in_millis": 1553357146038,
      "end_time": "2019-03-23T16:05:46.364Z",
      "end_time_in_millis": 1553357146364,
      "duration_in_millis": 326,
      "failures": [],
      "shards": {
        "total": 5,
        "failed": 0,
        "successful": 5
      },
      "feature_states": []
    }
  ],
  "total": 2,
  "remaining": 0
}
~~~
이 중에서 default 스냅숏을 복구한다. 터미널에 다음명령을 실행한다.
~~~
curl -XPOST 'http://localhost:9200/_snapshot/apache-web-log/default/_restore'
~~~
인덱스가 잘 생성됐는지 확인해보자.
정상적으로 생성됐다면 다음과 같이 apache-web-log 인덱스가 생성되며 총 10,001건의 로그가 존재한다.
~~~
GET /_cat/indices/apache*?v&pretty

health status index          uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   apache-web-log zkKZfOw6RaiFKPM1iMCbkQ   5   1      10001            0      8.8mb          8.8mb
~~~
맨 먼저 집계해볼 데이터는 지역별 사용자의 접속 수다. 
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "match_all": {
    }
  },
  "aggs": {
    "region_count": {
      "terms": {
        "field": "geoip.region_name.keyword",
        "size": 20
      }
    }
  }
}
~~~
집계에서 사용하는 필드 중 문자열 형태의 필드를 사용한다면 Keyword 타입으로 지정해야 한다.  
Keyword 타입은 Text 타입과 달리 분석 과정을 수행하지 않기 때문에 집계 성능이 향상된다.  
예제에서는 geoip.region_name.keyword와 같이 지역명에 대해 Keyword 타입을 지정했다.

집계쿼리를 실행한 결과는 다음과 같다.
~~~
{
  "took": 131,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 10000,
      "relation": "gte"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "region_count": {
      "doc_count_error_upper_bound": 39,
      "sum_other_doc_count": 3548,
      "buckets": [
        {
          "key": "California",
          "doc_count": 756
        },
        {
          "key": "Texas",
          "doc_count": 588
        },
        {
          "key": "Virginia",
          "doc_count": 424
        },
        {
          "key": "Pennsylvania",
          "doc_count": 355
        },
        {
          "key": "Washington",
          "doc_count": 273
        }
        ... (생략) ...,       
      ]
    }
  }
}
~~~
집계를 사용하면 쉽고 빠르게 필드를 그룹으로 묶고 통계 결과를 도출할 수 있다.

### 5.1.4 Aggregation API 이해하기
서비스를 운영하다 보면 데이터 필드의 값을 더하거나 평균을 내는 등 검색 쿼리로 반환된 데이터를 집계하는 경우가 많다.  
검색 쿼리의 결과 집계는 다음과 같이 기존 검색 쿼리에 집계 구문을 추가하는 방식으로 수행할 수 있다.
~~~
{
    "query": {...생략...},
    "aggs": {...생략...}
}
~~~
엘라스틱 서치는 집계 시 문서를 평가한 후 기준에 만족하는 문서들을 하나로 그룹화한다.  
그룹화한 집합을 토대로 집계를 수행하고, 집계가 끝나면 다음과 같이 버킷 목록에 속한 문서의 집합이 출력된다.
~~~
...생략...
"aggregations": {
    "region_count": {
      "doc_count_error_upper_bound": 39,
      "sum_other_doc_count": 3548,
      "buckets": [
        {
          "key": "California",
          "doc_count": 756
        },
        {
          "key": "Texas",
          "doc_count": 588
        },
        {
          "key": "Virginia",
          "doc_count": 424
        },
        {
          "key": "Pennsylvania",
          "doc_count": 355
        },
        {
          "key": "Washington",
          "doc_count": 273
        }
        ... (생략) ...,       
      ]
    }
  }
}
~~~
~~~
- 버킷 집계: 쿼리 결과로 도출된 도큐먼트 집합에 대해 특정 기준으로 나눈 다음 나눠진 도큐먼트들에 대한 산술 연산을 수행한다.
이때 나눠진 도큐먼트들의 모음들이 각 버킷에 해당된다.

- 메트릭 집계: 쿼리 결과로 도출된 도큐먼트 집합에서 필드의 값을 더하거나 평균을 내는 등의 산술 연산을 수행한다.

- 파이프라인 집계: 다른 집계 또는 관련 메트릭 연산의 결과를 집계한다.

- 행렬 집계: 버킷 대상이 되는 도큐먼트의 여러 필드에서 추출한 값으로 행렬 연산을 수행한다. 이를 토대로 다양한 통계정보를 제공하고 있으나
아직은 공식적인 집계 연산으로 제공되지 않고 실험적인 기능으로 제공되기 때문에 사용할 때 주의해야 한다.
~~~
엘라스틱 서치가 강력한 이유는 집계를 중첩해서 사용할 수 있다는 데 있다.  
하위 집계가 상위 집계의 버킷을 다시 집계하는 식이다.  
예컨데 상위 집계에서 date_histogram 집계로 일자별로 집계한 후 그 결과를 메트릭 집계로 다시 합산해 결과를 도출할 수 있다.  
중첩 횟수에 제한은 없으나 중첩할수록 성능 하락이 뒤따른다는 점에 주의해야 한다.

- 집계구문의 구조  
엘라스틱서치에서 제공하는 집계 연산을 사용하기 위해서는 문법적인 구조를 이해해야 한다.  
기본적인 집계 구문의 구조를 알아보자. 기본적인 구조는 다음과 같다.
~~~
"aggreagtions" : {
    "<aggregation_name>" : {
        "<aggregation_type>" : {
            <aggregation_body>
        }
        [,"meta" : { [ <meta_data_body> ] } ]?
        [, "aggregations" : { [ <sub_aggregation> ]+ }]?
    }
    [,"<aggregation_name_2>" : { ... } ]*
}
~~~
데이터를 집계하기 위해서는 맨 먼저 "aggregations"라는 단어를 명시해야 한다.  
"aggregations" 대신 "aggs"로 줄여서 쓰는 것도 가능하다.
"aggregation_name"에는 하위 집계의 이름을 기입한다. 이 이름은 집계의 결과 출력에 사용된다. 따라서 사용자가 직접 적당한 임의의 이름을 지정하다.  

"aggregation_type"에는 집계의 유형을 적는다. 어떤 집계를 수행할 것인가를 나타내는데, terms,date_histogram, sum 과 같은 다양한 집계 유형을 지정할 수 있다.  
"aggregation_body"에는 앞서 지정한 aggregation_type에 맞춰 내용을 작성하면 된다.

또한 meta 필드를 사용하거나 aggregations를 중첩할 수 있는데, 중첩의 경우 같은 레벨(aggregation_name_2)에 또 다른 집계를 정의하는 것도 가능하다.  
단, 같은 레벨에 집계를 정의할 때는 부수적인 성격의 집계만 정의할 수 있다.

- 집계 영역(Aggregation Scope)  
집계를 질의와 함계 수행하면 질의의 결과 영역 안에서 집계가 수행된다. 즉, 질의를 통해 반환된 문서들의 집합 내에서 집계를 수행하게 된다.
~~~
{
1."query": {
    "constant_score": {
        "filter" : {
            "match" : <필드 조건>
        }
    }
  },
2."aggs": {
    "<집계 이름>": {
      "<집계 타입>": {
        "field": "<필드명>"
      }
    }
  }
}
~~~
1. query: 질의를 수행한다. 하위에 필터 조건에 의해 명시한 필드와 값이 일치하는 문서만 반환한다.
2. aggs: 질의를 통해 반환받은 문서들의 집합 내에서 집계를 수행한다.

만약 질의가 생략된다면 내부적으로 match_all 쿼리로 수행되어 전체 문서에 대해 집계가 수행된다.
~~~
{
1."size" : 0,
2.  "aggs" : {
      "<집계 이름>": {
          "<집계 타입>": {
            "field": "<필드명>"
          }
      }
  }
}
~~~
1. size: 질의가 명시돼 있지 않기 때문에 내부적으로는 match_all 이 수행되고 size가 0이기 때문에결과 집합에 문서들 또한 존재하지 않는다. 즉, 문서의 결과는 출력되지 않는다.
2. aggs: 결과 문서가 출력되지 않더라도 실제 검색된 문서의 대상 범위가 전체 문서이기 때문에 집계는 전체 문서에 대해 수행된다.

한 번의 집계를 통해 질의에 해당하는 문서들 내에서도 집계를 수행하고 전체 문서에 대해서도 집계를 수행해야 하는 경우  
글로벌 버킷을 사용하면 질의 내에서도 전체 문서를 대상으로 집계를 수행할 수 있다.
~~~
{
  "query" : {
    "constant_score" : {
      "filter" : {
        "match" : <필드 조건>
      }
    }     
  },
1."aggs" : {
    "<집계 이름>" : {
      "<집계 타입>" : {
        "field" : "<필드명>"
      }
    },
    "<집계 이름>" : {
2.    "global" : {},
      "aggs" : {
        "<집계 이름>" : {
          "<집계 타입>" : {
            "field" : "<필드명>"
          }
        }
      }
    }
  }
}
~~~

1. 일반 버킷: 질의 영역 내에서만 집계를 수행
2. 글로벌 버킷: 전체문서를 대상으로 집계를 수행

## 5.2 메트릭 집계
메트릭 집계(Metrics Aggregations)를 사용하면 특정 필드에 대해 합이나 평균을 계산하거나  
다른 집계와 중첩해서 결과에 대해 특정 필드의 _score 값에 따라 정렬을 수행하거나  
지리 정보를 통해 범위 계산을 하는 등의 다양한 집계를 수행할 수 있다. 정수 또는 실수와 같이 숫자 연산을 할 수 있는 값들에 대한 집계를 수행한다.  
일반적으로 필드 데이터를 사용해 집계가 이뤄지지만 스크립트를 사용해 조금 더 유연하게 집계를 수행할 수도 있다.

메트릭 집계 내에서도 단일 숫자 메트릭 집계 와 다중 숫자 메트릭 집계로 나뉘는데,  
단일 숫자 메트릭 집계는 집계를 수행한 결괏값이 하나라는 의미로서 sum과 avg 등이 이에 속한다.  
다중 숫자 메트릭 집계는 집계를 수행한 결괏값이 여러 개가 될 수 있고, stats 나 geo_bounds가 이에 속한다.

예제에 사용된 쿼리는 전부 전체 데이터에 대해 집계를 수행하는데, 전체 데이터에 대한 쿼리라면 "query" 부분을 생략할 수 있다.  
또한 집계 결과가 아닌 검색 결과의 내용을 볼 필요가 없는 경우에는 size 값을 0으로 지정해서 검색 결과가 반환되지 않게 할 수도 있다.  
예제에서는 이런 방식을 사용해서 불필요한 결과는 노출하지 않게 할 것이다.

집계 요청에 사용될 공통적인 구조를 살펴본다.
~~~
1.  GET /apache-web-log/_search?size=0
    {
2.    "aggs": {
3.      "<집계 이름>": {
4.        "<집계 타입>": {
5.          "field": "<필드명>"
          }
        }
      }
}
~~~
1. size: 집계된 문서들의 데이터는 불필요하므로 size 값을 0으로 지정해 반환받지 않는다.
2. aggs: 집계를 수행한다. aggregation 또는 aggs를 입력할 수 있다.
3. 집계 이름: 집계에 대한 이름이다. 하위 쿼리 또는 여러 쿼리를 함께 사용할 때 구별하는 용도로 사용한다.
4. 집계 타입: 합계, 평균, 시계열 드으이 집계 타입을 명시한다.
5. field: 집계의 대상이 되는 필드를 명시한다.

집계 결과의 공통적인 구조를 살펴본다.
~~~
    {
1.    "took": 1,
2.    "time_out": false,
3.    "_shard": {
4.      "total": 5,
5.      "successful": 5,
6.      "skipped": 0,
7.      "failed": 0,
      },
8.    "hits": {
9.      "total": 200,
10.       "max_score": 0,
11.       "hits": [ ]
      },
12.     "aggreations": {
13.       "<집계 이름>": {
14.         <집계 결과>
        }
      }
    }
~~~
1. took: 엘라스틱서치가 검색을 실행하는 데 소요된 시간(ms)
2. time_out: 검색시간이 초과됐는지 여부
3. _shard: 검색에 영향받은 샤으데 대한 정보
4. _shard.total: 검색에 영향받은 샤드의 총 개수
5. _shard.successful: 검색 요청에 대한 처리를 정상 수행한 샤드 수
6. _shard.skipped: 검색 요청에 대한 처리를 건너 뛴 샤드 수
7. _shard.failed: 검색 요청에 대한 처리를 실패한 샤드 수
8. hits: 검색 결과
9. hits.total: 검색 기준과 일치하는 총 문서 수
10. hits.max_score: 검색 결과에 포함된 문서의 최대 스코어 값
11. hits.hits: 검색 결과 문서들의 배열(기본적으로 10개의 문서를 반환)
12. aggregations:집계 결과
13. 집계 이름:검색을 요청할 때 지정한 집계의 이름
14. 집계 결과:검색을 요청할 때 지정한 집계 타입에 따른 결과

### 5.2.1 합산 집계
합산집계(Sum Aggregation)는 단일 숫자 메트릭 집계에 해당한다.  
이를 통해 해당 서버로 총 얼마만큼의 데이터가 유입됐는지 집계해 보자.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "total_bytes": {
      "sum": {
        "field": "bytes"
      }
    }
  }
}
~~~
다음과 같이 집계 결과가 반환된다. 이를 통해 현재 서버로 총 얼마만큼의 데이터(Bytes)가 유입됐는지 쉽게 확인할 수 있다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "total_bytes": {
      "value": 2.747282505E9
    }
  }
}
~~~

이번에는 filter 기능을 사용해 특정 지역에서 유입된 데이터의 합을 계산해 본다.
~~~
GET /apache-web-log/_search?size=0
{
1."query": {
2.  "constant_score": {
3.    "filter": {
4.      "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "total_bytes": {
      "sum": {
        "field": "bytes"
      }
    }
  }
}
~~~
1. query: 쿼리 컨텍스트를 의미한다.
2. constant_score: 필터에 해당하는 문서들에 대해 동일한 스코어를 부여한다.
3. filter: 필터 컨텍스트를 의미한다.
4. match: geoip.city_name 필드 값이 'Paris'인 문서를 검색한다.

결과를 확인해본다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "total_bytes": {
      "value": 428964.0
    }
  }
}
~~~
필터 쿼리를 사용하면 위와 같이 파리에서 유입된 데이터 총량을 간단하게 확인할 수 있다.  
유입되는 데이터가 많아질수록 데이터의 크기가 커질 것이기 때문에 나중에는 데이터 총량이 어마어마하게 커질수도 있다.  
이럴 때 KB나 MB, GB 단위로 보고싶다면 script를 활용하여 집계되는 데이터를 원하는 단위로 변환할 수 있다.

다음과 같이 script를 사용해 합 연산을 수행해보자.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "total_bytes": {
      "sum": {
1.      "script": {
2.        "lang": "painless",
          "source": "doc.bytes.value"
        }
      }
    }
  }
}
~~~
1. script: 스크립트 컨텍스트를 의미하며, 6.x 부터는 그루비(Groovy)가 페인리스(Painless)를 기본 언어로 사용한다.
2. lang: 페인리스언어를 사용하는 경우 기본값이기 때문에 따로 명시하지 않아도 되지만 위 예제에서는 언어 확인을 위해 명시했다.
3. source: bytes 필드를 사용하려면 doc 객체의 bytes 속성 변수를 사용해야 하고, 값을 얻기 위해 bytes 객체의 value 속정변수를 사용해야 한다. 이에 대해서는 페인리스부분에서 자세히 다룬다.

결과는 앞에서 살펴본 합산 집계외 동일하다.

script를 사용하면 기존에 합산만 수행했던 것에서 더 나아가 다양한 연산을 수행할 수 있다.  
여기서는 KB로 나타내기 위해 1000으로 나눌 것이다(정확히는 1024로 나눠야 하지만 계산을 간단히 하기 위해 1000으로 나눈다).
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "total_bytes": {
      "sum": {
        "script": {
          "lang": "painless",
1.        "source": "doc.bytes.value / params.divide_value",
2.        "params": {
3.          "divide_value": 1000
          }
        }
      }
    }
  }
}
~~~
1. byte 필드의 값을 script 내의 params(변수와 같은 의미)에 명시한 값으로 나눈다.
2. script 내에서 사용할 파라미터 값들을 정의한다.
3. divide_value 파라미터의 값으로 1000을 대입한다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "total_bytes": {
      "value": 422.0
    }
  }
}
~~~
기존에는 결과가 428964 였는데 422가 됐다.  
1000으로 나눴을 때 예상했던 값은 428이었는데 422가 됐다. 그 이유는 1000으로 나누는 것은 모든 합산 값에 대한 나누기가 아니라  
각 문서의 개별적인 값을 1000으로 나눈 것이기 때문에 1000보다 작은 수들은 전부 0이 됐기 때문이다.  

이 문제를 해결하기 위해서는 정수가 아닌 실수로 계산해서 소수점까지 합산해야 한다. 이를 위해 다음과 같이 캐스팅 연산을 수행한다.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "total_bytes": {
      "sum": {
        "script": {
          "lang": "painless",
          "source": "doc.bytes.value / (double)params.divide_value",
          "params": {
            "divide_value": 1000
          }
        }
      }
    }
  }
}
~~~
params.divide_value 값을 double 로 형변환 했다. 결과는 다음과 같다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "total_bytes": {
      "value": 428.96399999999994
    }
  }
}
~~~
이처럼 script를 이용하면 집계 시 더 다양한 연산을 추가적으로 수행할 수 있기 때문에 유용하다.

### 5.2.2 평균 집계
평균 집계는 단일 숫자 메트릭 집계에 해당한다.  
이를 통해 해당 서버로 유입된 데이터의 평균 값을 쉽게 구할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "avg_bytes": {
      "avg": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
 ... 생략 ...
  "aggregations": {
    "avg_bytes": {
      "value": 294456.8601286174
    }
  }
}
~~~

이번에는 filter 기능을 사용해 특정 지역에서 유입된 데이터의 합을 계산해 보자.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "avg_bytes": {
      "avg": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "avg_bytes": {
      "value": 20426.85714285714
    }
  }
}
~~~

### 5.2.3 최솟값 집계
최솟값 집계(Min Aggregation)는 단일 숫자 메트릭 집계에 해당한다.  
서버로 유입된 데이터중 가장 작은 값을 쉽게 구할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "min_bytes": {
      "min": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "min_bytes": {
      "value": 35.0
    }
  }
}
~~~

이번에는 filter 기능을 사용해 특정 지역에서 유입된 데이터 중 가장 작은 값을 찾아본다.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "min_bytes": {
      "min": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "min_bytes": {
      "value": 1015.0
    }
  }
}
~~~

### 5.2.4 최댓값 집계
최댓값 집계(Max Aggregations)는 단일 숫자 메트릭 집계에 해당한다. 서버로 유입된 데이터중 가장 큰 값을 쉽게 구할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "max_bytes": {
      "max": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "max_bytes": {
      "value": 6.9192717E7
    }
  }
}
~~~

### 5.2.5 개수 집계
개수 집계(Value Count Aggregation)는 단일 숫자 메트릭 집계에 해당한다. 이를 통해 해당 서버로 사용자 요청이 몇 회 유입됐는지 쉽게 구할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "bytes_count": {
      "value_count": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_count": {
      "value": 9330
    }
  }
}
~~~

### 5.2.6 통계 집계
통계 집계(Stats Aggregation)는 결괏값이 여러 개인 다중 숫자 메트릭 집계에 해당한다.  
통계 집계를 사용하면 앞서 살펴본 합, 평균, 최대/최솟값, 개수를 한번의 쿼리로 집계할 수 있다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "bytes_stats": {
      "stats": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과는 다음과 같다..(열받네 이걸 이제알려주네 ㅡㅡ)
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_stats": {
      "count": 9330,
      "min": 35.0,
      "max": 6.9192717E7,
      "avg": 294456.8601286174,
      "sum": 2.747282505E9
    }
  }
}
~~~
이번에는 filter 기능을 사용해 특정 지역에 대한 통계를 집계해 보자.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "bytes_stats": {
      "stats": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_stats": {
      "count": 21,
      "min": 1015.0,
      "max": 53270.0,
      "avg": 20426.85714285714,
      "sum": 428964.0
    }
  }
}
~~~

### 5.2.7 확장 통계 집계
확장 통계 집계(Extended Stats Aggregation)는 결괏값이 여러 개인 다중 숫자 메트릭 집계에 해당한다.  
앞서 살펴본 통계 집계를 확장해서 표준편차 같은 통곗값이 추가됐다.
~~~
GET /apache-web-log/_search?size=0
{
  "aggs": {
    "bytes_extended_stats": {
      "extended_stats": {
        "field": "bytes"
      }
    }
  }
}
~~~
쿼리를 수행하면 다음과 같이 여러개의 결과를 포함한 집계가 반환된다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_extended_stats": {
      "count": 9330,
      "min": 35.0,
      "max": 6.9192717E7,
      "avg": 294456.8601286174,
      "sum": 2.747282505E9,
1.    "sum_of_squares": 1.18280314234513344E17,
2.    "variance": 1.2590713617814016E13,
      "variance_population": 1.2590713617814016E13,
      "variance_sampling": 1.2592063249459188E13,
3.    "std_deviation": 3548339.5578515334,
      "std_deviation_population": 3548339.5578515334,
      "std_deviation_sampling": 3548529.730671449,
4.    "std_deviation_bounds": {
5.      "upper": 7391135.975831684,
6.      "lower": -6802222.25557445,
        "upper_population": 7391135.975831684,
        "lower_population": -6802222.25557445,
        "upper_sampling": 7391516.321471516,
        "lower_sampling": -6802602.60121428
      }
    }
  }
}
~~~
1. sum_of_squares: 제곱합을 의미한다. 제곱합은 변동의 측정이나 평균의 편차를 나타낸다.
2. variance: 확률변수의 분산을 의미한다. 분산은 확률 변수가 기댓값으로부터 얼마나 떨어진 곳에 분포하는지를 가늠 하는 숫자다(편차 제곱의 평균.)
3. std_deviation: 표준편차를 의미한다. 표준편차는 자료의 산포도를 나타내는 수치로서, 분산의 양의 제곱근으로 정의된다.
4. std_deviation_bounds: 표준편차의 번위를 의미한다.
5. upper: 표준편차의 상한 값을 의미한다.
6. lower: 표준편차의 하한 값을 의미한다.

이번에는 filter 기능을 사용해 특정 지역에 대한 통계를 집계해 보자.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.city_name": {
            "query": "Paris"
          }
        }
      }
    }
  },
  "aggs": {
    "bytes_extended_stats": {
      "extended_stats": {
        "field": "bytes"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "bytes_extended_stats": {
      "count": 21,
      "min": 1015.0,
      "max": 53270.0,
      "avg": 20426.85714285714,
      "sum": 428964.0,
      "sum_of_squares": 1.8371748404E10,
      "variance": 4.575886693605442E8,
      "variance_population": 4.575886693605442E8,
      "variance_sampling": 4.8046810282857144E8,
      "std_deviation": 21391.32229107271,
      "std_deviation_population": 21391.32229107271,
      "std_deviation_sampling": 21919.582633539612,
      "std_deviation_bounds": {
        "upper": 63209.501725002556,
        "lower": -22355.787439288277,
        "upper_population": 63209.501725002556,
        "lower_population": -22355.787439288277,
        "upper_sampling": 64266.02240993637,
        "lower_sampling": -23412.308124222083
      }
    }
  }
}
~~~

### 5.2.8 카디널리티 집계
카디널리티 집계(Cardinality Aggregation)는 단일 숫자 메트릭 집계에 해당한다.  
개수 집합과 유사하게 횟수를 계산하는데, 중복된 값은 제외한 고유한 값에 대한 집계를 수행한다.  
하지만 모든 문서에 대해 중복된 값을 집계하는 것은 성능에 큰 영향을 줄 수 있기 때문에 근사치를 통해 집계를 수행한다.

아파치 웹 로그 예제에서 미국의 몇 개 도시에서 데이터 유입이 있었는지 횟수를 집계해 보자.  
먼저 아직 살펴보진 않았지만 terms 집계를 통해 미국의 어느 지역에서 데이터 유입이 있었는지 확인해 보겠다.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
1.      "match": {
          "geoip.country_name": {
            "query": "United States"
          }
        }
      }
    }
  },
  "aggs": {
    "us_city_names": {
2.    "terms": {
3.      "field": "geoip.city_name.keyword"
      }
    }
  }
}
~~~
1. match: 국가명이 "United States"인 문서만 필터링한다.
2. terms: 텀즈 쿼리를 사용한다.
3. field: 개수를 집계할 필드를 명시한다. 각 문서에 명시된 필드별로 개수가 집계된다.  
string 타입의 필드의 경우 text 타입과 keyword 타입의 두 타입이 공존하게 되는데, text 타입의 경우 텍스트 검색용으로 사용하기 위해  
분석기를 수행한 후 색인되기 때문에 집계할 때는 keyword 타입을 사용한다.

쿼리를 수행하면 다음과 같이 각 지역별로 개수가 집계되어 반환된다.
~~~
{
  ... 생략 ...
  "aggregations": {
    "us_city_names": {
      "doc_count_error_upper_bound": 30,
      "sum_other_doc_count": 1955,
      "buckets": [
        {
          "key": "Leander",
          "doc_count": 539
        },
        {
          "key": "Lititz",
          "doc_count": 273
        },
        {
          "key": "San Francisco",
          "doc_count": 230
        },
        {
          "key": "Ashburn",
          "doc_count": 153
        },
        ... 생략 ...
      ]
    }
  }
}
~~~
집계된 결과를 살펴보면 미국 내에서 요청 수가 가장 많은 도시 순으로 결과가 반환된다.  
그렇다면 미국 내 몇 개의 도시에서 유입이 있었는지 확인하고 싶다면 어떻게 해야할까?  
지금까지의 집계로는 동일한 필드 값에 대해서도 집계 연산이 수행되기 때문에 알아낼 수가 없었다.  
이러한 경우는 다음과 같이 카디널리티 집계를 사용한다.
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "constant_score": {
      "filter": {
        "match": {
          "geoip.country_name": {
            "query": "United States"
          }
        }
      }
    }
  },
  "aggs": {
    "us_cardinality": {
      "cardinality": {
        "field": "geoip.city_name.keyword"
      }
    }
  }
}
~~~
결과
~~~
{
  ... 생략 ...
  "aggregations": {
    "us_cardinality": {
      "value": 249
    }
  }
}
~~~
앞에서 언급한 것처럼 카디널리티 집계는 대상 필드를 고유한 값으로 근사치를 계산하는 메트릭 집계다.  
데이터가 적은 경우 거의 정확한 결과를 확인할 수 있지만 기본적으로 근사치 계산이라는 점에 주의하자.  
위 결과를 통해 미국 내 249개의 도시에서 요청이 있었음을 알 수 있다.
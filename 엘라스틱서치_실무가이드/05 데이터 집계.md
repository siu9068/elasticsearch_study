## 5.1 집계

### 5.1.1 엘라스틱서치와 데이터 분석
일반적인 통게 프로그램은 배치 방식으로 데이터를 처리한다. 대용량 데이터를 하둡이나 관계형 데이터베이스에 적재하고 배치로 처리하는 식이다.  
반면 엘라스틱 서치는 많은 양의 데이터를 조각내어 관리한다. 덕분에 문서의 수가 늘어나도 배치 처리보다 좀 더 실시간에 가깝게 문서를 처리할 수 있다.  

SQL로는 다음과같이 GROUP BY 구문을 이용해 집계할 수 있다.
~~~
SELECT SUM(ratings) FROM movie_review GROUP BY movie_no;
~~~
엘라스틱 서치의 Query DSL로 집계하는 쿼리는 다음과 같다.
~~~
POST /movie_search/_search
{
  "aggs": {
    "movie_no_agg": {
      "terms": {
        "field": "movie_no"
      },
      "aggs": {
        "ratings_agg": {
          "sum": {
            "field": "ratings"
          }
        }
      }
    }
  }
}
~~~
엘라스틱서치는 SQL보다 더욱 강력한 집계 기능을 제공한다.  
집계를 여러 개 중첩해 사용할 수 있을뿐더러 범위, 날짜, 위치 정보도 집꼐할 수 있다.  
또한 엘라스틱서치는 인덱스를 활용해 분산 처리가 가능하기 때문에 SQL보다 더 많은 데이터를 빠르게 집계할 수 있다.

### 5.1.2 엘라스틱서치가 집계에 사용하는 기술
#### 캐시  
집계 쿼리로 값을 조회하면, 여러 노드에 있는 데이터를 집계해 질의에 답하기 때문에 데이터의 양이 클수록 CPU와 메모리 자원이 많이 소모된다.  
질의에 응답하는 시간 또한 길어진다.  
이런 문제는 노드의 하드웨어 성능을 높이면 해결할 수 있지만 하드웨어 업그레이드는 막대한 비용이 들기 때문에 올바른 해결책이 아니다.  
가장 현실적이고 비용 효율적인 결책은 '캐시'를 이용하는 것이다.  
캐시는 질의결과를 임시 버퍼(캐시)에 둔다. 이후 처리해야 하는 같은 질의에 대해 매번 결과를 계산하는 게 아니라 버퍼에 보관된 결과를 반환한다.  
캐시를 적용하는 것만으로도 인덱스의 성능을 대폭 향상시킬 수 있다.  
캐시의 크기는 일반적으로 힙 메모리의 1%를 할당하며, 캐시에 없는 질의의 경우 성능 향상에 별다른 도움이 되지 않는다.  

캐시는 엘라스틱서치의 conf 폴더 안의 elasticsearch.yml 파일을 수정해 활성화할 수 있다.  
다음은 힙메모리의 2%를 캐시에 할당하는 예다.
~~~
indices.requests.cache.size: 2%
~~~

- Node query Cache  
노드의 모든 샤드가 공유하는 LRU(Least-Recently-Used) 캐시다. 캐시 용량이 가득차면 사용량이 가장 적은 데이터를 삭제하고 새로운 결괏값을 캐싱한다.  
기본적으로 10%의 필터 캐시가 매모리를 제어하고 쿼리 캐싱을 사용할지 여부는 elasticsearch.yml 파일에 옵션으로 추가하면 된다. 기본값은 true
~~~
index.queries.cache.enabled:true
~~~

- Shard request Cache  
엘라스틱서치는 인덱스의 수평적 확산과 병렬 처리를 통한 성능 향상을 위해 고안된 개념이다.  
샤드는 데이터를 분산 저장하기 위한 단위로서, 그 자체가 온전한 기능을 가진 독립 인덱스라고 할 수 있다.  
- Shard request Cache는 바로 이 샤드에서 수행된 쿼리의 결과를 캐싱한다.   
샤드의 내용이 변경되면 캐시가 삭제되기 때문에 업데이트가 빈번한 인덱스에서는 오히려 성능 저하를 일으킬 수 있다.

- Field data Cache  
엘라스틱서치가 필드에서 집계 연산을 수행할 때는 모든 필드 값을 메모리에 로드한다.  
이러한 이유로 엘라스틱서치에서 계산되는 집계 쿼리는 성능적인 측면에서 비용이 많이든다.  
Field data Cache는 집계가 계산되는 동안 필드의 값을 메모리에 보관한다.

### 5.1.3 실습 데이터 살펴보기
스냅숏의 목록을 확인해본다.
~~~
curl -XGET 'http://localhost:9200/_snapshot/apache-web-log/_all?pretty'
~~~
apache-web-log 스냅숏 그룹 내부에는 다음과 같이 2개의 스냅숏이 존재한다.
~~~
{
  "snapshots": [
    {
      "snapshot": "default",
      "uuid": "yzmzEx6uSMS55j60z4buBA",
      "repository": "apache-web-log",
      "version_id": 6040399,
      "version": "6.4.3",
      "indices": [
        "apache-web-log"
      ],
      "data_streams": [],
      "include_global_state": false,
      "state": "SUCCESS",
      "start_time": "2019-03-23T16:03:50.351Z",
      "start_time_in_millis": 1553357030351,
      "end_time": "2019-03-23T16:03:50.604Z",
      "end_time_in_millis": 1553357030604,
      "duration_in_millis": 253,
      "failures": [],
      "shards": {
        "total": 5,
        "failed": 0,
        "successful": 5
      },
      "feature_states": []
    },
    {
      "snapshot": "applied-mapping",
      "uuid": "SgXhqApiSHiauC6fbjSHMw",
      "repository": "apache-web-log",
      "version_id": 6040399,
      "version": "6.4.3",
      "indices": [
        "apache-web-log-applied-mapping"
      ],
      "data_streams": [],
      "include_global_state": false,
      "state": "SUCCESS",
      "start_time": "2019-03-23T16:05:46.038Z",
      "start_time_in_millis": 1553357146038,
      "end_time": "2019-03-23T16:05:46.364Z",
      "end_time_in_millis": 1553357146364,
      "duration_in_millis": 326,
      "failures": [],
      "shards": {
        "total": 5,
        "failed": 0,
        "successful": 5
      },
      "feature_states": []
    }
  ],
  "total": 2,
  "remaining": 0
}
~~~
이 중에서 default 스냅숏을 복구한다. 터미널에 다음명령을 실행한다.
~~~
curl -XPOST 'http://localhost:9200/_snapshot/apache-web-log/default/_restore'
~~~
인덱스가 잘 생성됐는지 확인해보자.
정상적으로 생성됐다면 다음과 같이 apache-web-log 인덱스가 생성되며 총 10,001건의 로그가 존재한다.
~~~
GET /_cat/indices/apache*?v&pretty

health status index          uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   apache-web-log zkKZfOw6RaiFKPM1iMCbkQ   5   1      10001            0      8.8mb          8.8mb
~~~
맨 먼저 집계해볼 데이터는 지역별 사용자의 접속 수다. 
~~~
GET /apache-web-log/_search?size=0
{
  "query": {
    "match_all": {
    }
  },
  "aggs": {
    "region_count": {
      "terms": {
        "field": "geoip.region_name.keyword",
        "size": 20
      }
    }
  }
}
~~~
집계에서 사용하는 필드 중 문자열 형태의 필드를 사용한다면 Keyword 타입으로 지정해야 한다.  
Keyword 타입은 Text 타입과 달리 분석 과정을 수행하지 않기 때문에 집계 성능이 향상된다.  
예제에서는 geoip.region_name.keyword와 같이 지역명에 대해 Keyword 타입을 지정했다.

집계쿼리를 실행한 결과는 다음과 같다.
~~~
{
  "took": 131,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 10000,
      "relation": "gte"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "region_count": {
      "doc_count_error_upper_bound": 39,
      "sum_other_doc_count": 3548,
      "buckets": [
        {
          "key": "California",
          "doc_count": 756
        },
        {
          "key": "Texas",
          "doc_count": 588
        },
        {
          "key": "Virginia",
          "doc_count": 424
        },
        {
          "key": "Pennsylvania",
          "doc_count": 355
        },
        {
          "key": "Washington",
          "doc_count": 273
        }
        ... (생략) ...,       
      ]
    }
  }
}
~~~
집계를 사용하면 쉽고 빠르게 필드를 그룹으로 묶고 통계 결과를 도출할 수 있다.

### 5.1.4 Aggregation API 이해하기
서비스를 운영하다 보면 데이터 필드의 값을 더하거나 평균을 내는 등 검색 쿼리로 반환된 데이터를 집계하는 경우가 많다.  
검색 쿼리의 결과 집계는 다음과 같이 기존 검색 쿼리에 집계 구문을 추가하는 방식으로 수행할 수 있다.
~~~
{
    "query": {...생략...},
    "aggs": {...생략...}
}
~~~
엘라스틱 서치는 집계 시 문서를 평가한 후 기준에 만족하는 문서들을 하나로 그룹화한다.  
그룹화한 집합을 토대로 집계를 수행하고, 집계가 끝나면 다음과 같이 버킷 목록에 속한 문서의 집합이 출력된다.
~~~
...생략...
"aggregations": {
    "region_count": {
      "doc_count_error_upper_bound": 39,
      "sum_other_doc_count": 3548,
      "buckets": [
        {
          "key": "California",
          "doc_count": 756
        },
        {
          "key": "Texas",
          "doc_count": 588
        },
        {
          "key": "Virginia",
          "doc_count": 424
        },
        {
          "key": "Pennsylvania",
          "doc_count": 355
        },
        {
          "key": "Washington",
          "doc_count": 273
        }
        ... (생략) ...,       
      ]
    }
  }
}
~~~
~~~
- 버킷 집계: 쿼리 결과로 도출된 도큐먼트 집합에 대해 특정 기준으로 나눈 다음 나눠진 도큐먼트들에 대한 산술 연산을 수행한다.
이때 나눠진 도큐먼트들의 모음들이 각 버킷에 해당된다.

- 메트릭 집계: 쿼리 결과로 도출된 도큐먼트 집합에서 필드의 값을 더하거나 평균을 내는 등의 산술 연산을 수행한다.

- 파이프라인 집계: 다른 집계 또는 관련 메트릭 연산의 결과를 집계한다.

- 행렬 집계: 버킷 대상이 되는 도큐먼트의 여러 필드에서 추출한 값으로 행렬 연산을 수행한다. 이를 토대로 다양한 통계정보를 제공하고 있으나
아직은 공식적인 집계 연산으로 제공되지 않고 실험적인 기능으로 제공되기 때문에 사용할 때 주의해야 한다.
~~~
엘라스틱 서치가 강력한 이유는 집계를 중첩해서 사용할 수 있다는 데 있다.  
하위 집계가 상위 집계의 버킷을 다시 집계하는 식이다.  
예컨데 상위 집계에서 date_histogram 집계로 일자별로 집계한 후 그 결과를 메트릭 집계로 다시 합산해 결과를 도출할 수 있다.  
중첩 횟수에 제한은 없으나 중첩할수록 성능 하락이 뒤따른다는 점에 주의해야 한다.

- 집계구문의 구조  
엘라스틱서치에서 제공하는 집계 연산을 사용하기 위해서는 문법적인 구조를 이해해야 한다.  
기본적인 집계 구문의 구조를 알아보자. 기본적인 구조는 다음과 같다.
~~~
"aggreagtions" : {
    "<aggregation_name>" : {
        "<aggregation_type>" : {
            <aggregation_body>
        }
        [,"meta" : { [ <meta_data_body> ] } ]?
        [, "aggregations" : { [ <sub_aggregation> ]+ }]?
    }
    [,"<aggregation_name_2>" : { ... } ]*
}
~~~
데이터를 집계하기 위해서는 맨 먼저 "aggregations"라는 단어를 명시해야 한다.  
"aggregations" 대신 "aggs"로 줄여서 쓰는 것도 가능하다.
"aggregation_name"에는 하위 집계의 이름을 기입한다. 이 이름은 집계의 결과 출력에 사용된다. 따라서 사용자가 직접 적당한 임의의 이름을 지정하다.  

"aggregation_type"에는 집계의 유형을 적는다. 어떤 집계를 수행할 것인가를 나타내는데, terms,date_histogram, sum 과 같은 다양한 집계 유형을 지정할 수 있다.  
"aggregation_body"에는 앞서 지정한 aggregation_type에 맞춰 내용을 작성하면 된다.

또한 meta 필드를 사용하거나 aggregations를 중첩할 수 있는데, 중첩의 경우 같은 레벨(aggregation_name_2)에 또 다른 집계를 정의하는 것도 가능하다.  
단, 같은 레벨에 집계를 정의할 때는 부수적인 성격의 집계만 정의할 수 있다.

- 집계 영역(Aggregation Scope)  
집계를 질의와 함계 수행하면 질의의 결과 영역 안에서 집계가 수행된다. 즉, 질의를 통해 반환된 문서들의 집합 내에서 집계를 수행하게 된다.
~~~
{
1."query": {
    "constant_score": {
        "filter" : {
            "match" : <필드 조건>
        }
    }
  },
2."aggs": {
    "<집계 이름>": {
      "<집계 타입>": {
        "field": "<필드명>"
      }
    }
  }
}
~~~
1. query: 질의를 수행한다. 하위에 필터 조건에 의해 명시한 필드와 값이 일치하는 문서만 반환한다.
2. aggs: 질의를 통해 반환받은 문서들의 집합 내에서 집계를 수행한다.

만약 질의가 생략된다면 내부적으로 match_all 쿼리로 수행되어 전체 문서에 대해 집계가 수행된다.
~~~
{
1."size" : 0,
2.  "aggs" : {
      "<집계 이름>": {
          "<집계 타입>": {
            "field": "<필드명>"
          }
      }
  }
}
~~~
1. size: 질의가 명시돼 있지 않기 때문에 내부적으로는 match_all 이 수행되고 size가 0이기 때문에결과 집합에 문서들 또한 존재하지 않는다. 즉, 문서의 결과는 출력되지 않는다.
2. aggs: 결과 문서가 출력되지 않더라도 실제 검색된 문서의 대상 범위가 전체 문서이기 때문에 집계는 전체 문서에 대해 수행된다.

한 번의 집계를 통해 질의에 해당하는 문서들 내에서도 집계를 수행하고 전체 문서에 대해서도 집계를 수행해야 하는 경우  
글로벌 버킷을 사용하면 질의 내에서도 전체 문서를 대상으로 집계를 수행할 수 있다.
~~~
{
  "query" : {
    "constant_score" : {
      "filter" : {
        "match" : <필드 조건>
      }
    }     
  },
1."aggs" : {
    "<집계 이름>" : {
      "<집계 타입>" : {
        "field" : "<필드명>"
      }
    },
    "<집계 이름>" : {
2.    "global" : {},
      "aggs" : {
        "<집계 이름>" : {
          "<집계 타입>" : {
            "field" : "<필드명>"
          }
        }
      }
    }
  }
}
~~~

1. 일반 버킷: 질의 영역 내에서만 집계를 수행
2. 글로벌 버킷: 전체문서를 대상으로 집계를 수행
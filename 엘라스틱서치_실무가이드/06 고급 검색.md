## 6.1 한글 형태소 분석기 사용하기

### 6.1.1 은전한닢 형태소 분석기
은전한닢은 Mecab-ko-dic 기반으로 만들어진 한국어 형태소 분석기로서 자바 인터페이스와 스칼라 인터페이스 두 종류를 제공한다.  
시스템 사전에 등록돼 있는 사전을 기반으로 동작하며, 복합명사와 활용어의 원형 찾기가 가능하다.  
라이선스는 아파치 라이선스2.0을 채택했으며, 소프트웨어 사용 및 재배포에 대한 제약 없이 사용 가능하다.  
현재 엘라스틱서치에서 가장 보편적으로 사용하는 한글 형태소 분석기다.

~~~
PUT /seunjeon_default_analyzer
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 1,
    "index": {
      "analysis": {
        "analyzer": {
          "korean": {
            "type": "custom",
            "tokenizer":"seunjeon_default_tokenizer"
          }
        },
        "tokenizer": {
          "seunjeon_default_tokenizer": {
            "type": "seunjeon_tokenizer",
            "index_eojeol": false,
            "user_words": [
              "낄끼+빠빠,-100", "c\\+\\+", "어그로", "버카충", "abc마트"
              ]
          }
        }
      }
    }
  }
}
~~~
tokenizer 설정을 통해 은전한닢 플러그인에서 제공하는 seunjeon_tokenizer를 활성화하고 analyzer 설정을 통해 형태소 분석기를 정의하면 사용할 준비가 끝난다.

|      파라미터      | 설명                                                                                  |
|:--------------:|:------------------------------------------------------------------------------------|
|   user_words   | 사용자 사전을 정의한다(기본값: [])                                                               |
| user_dict_path | 사용자 사전 파일의 경로를 정의한다. 해당 파일은 엘라스틱 서치의 config 폴더 밑에 생성한다.                             |
|   decompoud    | 복합명사 분해 여부를 정의한다(기본값: true).                                                        |
|   deinflect    | 활용어의 원형을 추출한다(기본값: true).                                                           |
|  index_eojeol  | 어절을 추출한다(기본값: true).                                                                |
|  index_poses   | 추출할 품사를 정의한다. 품사의 정의는 다음 표를 참고한다 (예: "N", "SL", "SH", "SN", "XR", "V", "M", "UNK"). |
|  pos_tagging   | 품사 태깅 여부를 정의한다(키워드에 품사가 붙어져서 나온다.(기본값: true).                                       |
| max_unk_length | Unknown 품사 태깅의 키워드로 뽑을 수 있는 최대 길이를 정의한다(기본값: 8).                                    |

| 품사 태그명 | 설명        |
|:------:|:----------|
|  UNK   | 알 수 없는 단어 |
|   EP   | 선어말어미     |
|   E    | 어미        |
|   I    | 독립언       |
|   J    | 관계언/조사    |
|   M    | 수식언       |
|   N    | 체언        |
|   S    | 부호        |
|   SL   | 외국어       |
|   SH   | 한자        |
|   SN   | 숫자        |
|   V    | 용언        |
|  VCP   | 긍정지정사     |
|   XP   | 접두사       |
|   XS   | 접미사       |
|   XR   | 어근        |

#### 사전추가
한글에는 복합명사가 있다. 예를들어, "삼성전자" 같은 단어는 "삼성"과 "전자"라는 형태소로 분리되는 단어인데, 이처럼 다수의 단어를 하나로 합해서 하나의 단어처럼 사용하는 것을
복합 명사라 한다. 검색 엔진에서 "삼성"을 검색하거나 "전자"를 검색했을 때도 문서가 검색되게 하고 싶다면 색인할 때 복함명사를 분리해서 역색인 해야한다.  
혹은 반대로 복합명사를 단일명사처럼 단어가 분리되지 않게 하고 싶은 경우도 있을 것이다.  
은전한닢에서는 이러한 부분들을 해결할 수 있게 사용자가 등록하는 사전을 제공하는데 이러한 사전을 "사용자 사전"이라 한다.

사용자 사전의 위치는 인덱스를 생성한 후 setting에 사용자 사전의 경로를 지정하면 된다.  
이를 지정하는 파라미터는 user_dict_path 이며 설정하는 방법은 다음과 같다.

~~~
PUT /seunjeon_with_dic_analyzer
{
  "settings": {
    "index": {
      "analysis": {
        "tokenizer": {
          "seunjeon_default_tokenizer": {
            "index_eojeol": "false",
            "pos_tagging": "false",
            "user_dict_path": "dic/user_dic.csv",
            "type": "seunjeon_tokenizer"
          }
        },
        "analyzer": {
          "korean": {
            "filter": [
              "lowercase"
              ],
              "tokenizer": "seunjeon_default_tokenizer",
              "type": "custom"
          }
        }
      }
    }
  }
}
~~~

여기서는 경로를 dic/user_dic.csv로 지정했다.  
사용자 사전은 엘라스틱서치 서버가 설치된 디렉터리의 config 디렉터리 안에 dic 아래로 생성하면 된다.  
사용자 사전은 텀(Term)과 가중치(Weight) 형태로 구성돼 있으며 가중치의 값이 작을수록 그에 따른 우선위는 높아진다.  
예를 들어, 사용자 사전에 "삼성", "전자", "삼성전자"라는 단어가 존재할 경우 삼성전자의 우선순위를 다른 단어보다 더 높게 줘서 복합 명사가 분리되지 않게 할 수도있다.

사용자 사전을 등록할 때는 다음과 같은 형식으로 등록한다.
~~~
삼성전자,-100
삼성,-50
전자,-50
~~~

### 6.1.2 Nori 형태소 분석기
기존 현태소 분석기에 비해 30%이상 빠르고 메모리 용량도 현저하게 줄었으며, 시스템 전반에 영향을 주지 않게 최적화 됐다.

Nori는 서드파티 플러그인과 달리 루씬에서 공식적으로 지원되고 있지만 기본 플러그인으로는 포함돼있지 않다.  
따라서 엘라스틱서치에서 Nori를 사용하려면 다른 서드파티 플러그인과 마찬가지로 직접 설치해야 한다.  
설치 방법은 다음과 같다.
~~~
bin/elasticsearch-plugin install analysis-nori
~~~

Nori 분석기는 다음과 같이 하나의 토크나이저와 두 개의 토큰 필터로 구성돼 있으며 사용자 설정 사전과 불용어 처리를 위한 stoptags를 지원한다.
- nori_tokenizer: 토크나이저
- nori_part_of_speech: 토큰 필터
- nori_readingform: 토큰 필터

#### 6.1.2.1 nori_tokenizer 토크나이저
토크나이저는 형태소를 토큰 형태로 분리하는 데 사용한다. 다음과 같이 두 가지 파라미터를 지원한다.
- decompound_mode: 복합명사를 토크나이저가 처리하는 방식
- user_dictionary: 사용자 사전정의

1. decompound_mode  
decompound_mode는 토크나이저가 복합명사를 처리하는 방식을 결정한다. 복합명사가 있을 경우 단어를 어떻게 쪼갤지 결정한다.  
단어를 쪼개는 방법은 다음과 같이 세 가지 중에서 설정할 수 있다.

|      파라미터명      | 파라미터 값  |            설명            |       예제        |
|:---------------:|:-------:|:------------------------:|:---------------:|
| decompound_mode |  none   |      복합명사로 분리하지 않는다      |   월미도<br/>영종도   |
|                 | discard | 복합명사로 분리하고 원본 데이터는 삭제한다. |   잠실역=>[잠실,역]   |
|                 |  mixed  | 복합명사로 분리하고 원본 데이터는 유지한다. | 잠실역=>[잠실,역,잠실역] |


2. user_dictionary  
Nori 토크나이저 내부적으로 세종 말뭉치와 mecab-ko-dic 사전을 사용한다. user dictionary를 이용해 사용자가 정의한 명사를 사전에 추가로 등록할 수 있다.
user_dictionary는 엘라스틱서치 서버가 설치된 디렉터리 아래에 config/userdic_ko.txt 형태로 생성해서 사용할 수 있으며 인덱스 매핑 시 분석기의 파라미터로 사전 경로를 등록하면 된다.

userdic_ko.txt 파일에 명사를 추가하는 방법은 다음과 같다. 추가할 명사 혹은 복합명사의 구조로 설정하면 된다.
~~~
삼성전자
삼성전자 삼성 전자
~~~
다음은 decompound_mode를 "mixed"로 설정하고 userdict_ko.txt 사용자 사전을 추가한 예다.
~~~
PUT /nori_analyzer
{
  "settings": {
    "index": {
      "analysis": {
        "tokenizer": {
          "nori_user_dict_tokenizer": {
            "type": "nori_tokenizer",
            "decompound_mode": "mixed",
            "user_dictionary": "userdict_ko.txt"
          }
        },
        "analyzer": {
          "nori_token_analyzer": {
            "type": "custom",
            "tokenizer": "nori_user_dict_tokenizer"
          }
        }
      }
    }
  }
}
~~~
생성된 nori_analyzer 인덱스에 설정된 nori_token_analyzer를 테스트해 보자.
~~~
POST nori_analyzer/_analyze
{
  "analyzer": "nori_token_analyzer",
  "text": "잠실역"
}
~~~
"잠실역"이라는 단어를 분석해 보면 decompound의 값을 mixed로 설정했기 때문에 원본 데이터와 복합어로 나눠진 단어가 모두 출력되는 것을 볼 수 있다.
~~~
{
  "tokens" : [
    {
      "token" : "잠실역",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0,
      "positionLength" : 2
    },
    {
      "token" : "잠실",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "역",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "word",
      "position" : 1
    }
  ]
}
~~~

#### 6.1.2.2 nori_part_of_speech 토큰 필터

nori